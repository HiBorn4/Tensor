{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yl5kNRf_XRA",
        "outputId": "6df1b3ff-823c-4ae8-a259-977ee71350a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MultiBench'...\n",
            "remote: Enumerating objects: 6943, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 6943 (delta 72), reused 121 (delta 60), pack-reused 6789\u001b[K\n",
            "Receiving objects: 100% (6943/6943), 51.07 MiB | 17.78 MiB/s, done.\n",
            "Resolving deltas: 100% (4258/4258), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pliang279/MultiBench.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MultiBench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqO6uFph_3E-",
        "outputId": "59454c18-b795-4da4-d43c-ee32f4dbf83b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MultiBench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!pip install gdown && gdown https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po01UPmD_5jy",
        "outputId": "aa877cf7-993d-4a4c-bc11-d73c90736075"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU\n",
            "From (redirected): https://drive.google.com/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU&confirm=t&uuid=2ba79ccf-8145-440f-80a9-d01c403c9fab\n",
            "To: /content/MultiBench/mosi_raw.pkl\n",
            "100% 357M/357M [00:02<00:00, 144MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "OwGGbljm_8oi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.affect.get_data import get_dataloader\n",
        "\n",
        "# Load the dataset\n",
        "traindata, validdata, testdata = get_dataloader(\n",
        "    'mosi_raw.pkl', robust_test=False, max_pad=True, data_type='mosi', max_seq_len=50)"
      ],
      "metadata": {
        "id": "NbL06-7oAA1L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubNet(nn.Module):\n",
        "    '''\n",
        "    The subnetwork that is used in LMF for video and audio in the pre-fusion stage\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_size, hidden_size, dropout):\n",
        "        super(SubNet, self).__init__()\n",
        "        self.norm = nn.BatchNorm1d(in_size)\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "        self.linear_1 = nn.Linear(in_size, hidden_size)\n",
        "        self.linear_2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear_3 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        normed = self.norm(x)\n",
        "        dropped = self.drop(normed)\n",
        "        y_1 = F.relu(self.linear_1(dropped))\n",
        "        y_2 = F.relu(self.linear_2(y_1))\n",
        "        y_3 = F.relu(self.linear_3(y_2))\n",
        "\n",
        "        return y_3\n"
      ],
      "metadata": {
        "id": "HfkeQV3wAEZI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextSubNet(nn.Module):\n",
        "    '''\n",
        "    The LSTM-based subnetwork that is used in LMF for text\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_size, hidden_size, out_size, num_layers=1, dropout=0.2, bidirectional=False):\n",
        "        super(TextSubNet, self).__init__()\n",
        "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, final_states = self.rnn(x)\n",
        "        h = self.dropout(final_states[0].squeeze())\n",
        "        y_1 = self.linear_1(h)\n",
        "        return y_1\n"
      ],
      "metadata": {
        "id": "a2YNJi2ZAGrI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LMF(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, text_out, dropouts, output_dim, rank, use_softmax):\n",
        "        super(LMF, self).__init__()\n",
        "\n",
        "        # Unpack input_dims\n",
        "        audio_dim, video_dim, text_dim = input_dims\n",
        "\n",
        "        # Define audio network\n",
        "        self.audio_net = nn.Sequential(\n",
        "            nn.Linear(audio_dim, hidden_dims[0]),\n",
        "            nn.BatchNorm1d(hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropouts[0]),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "            nn.BatchNorm1d(hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropouts[1]),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.BatchNorm1d(hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropouts[2])\n",
        "        )\n",
        "\n",
        "        # Define video network\n",
        "        self.video_net = nn.Sequential(\n",
        "            nn.Linear(video_dim, hidden_dims[0]),\n",
        "            nn.BatchNorm1d(hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropouts[0]),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "            nn.BatchNorm1d(hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropouts[1]),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.BatchNorm1d(hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropouts[2])\n",
        "        )\n",
        "\n",
        "        # Define text network\n",
        "        self.text_net = nn.Sequential(\n",
        "            nn.Linear(text_dim, text_out),\n",
        "            nn.BatchNorm1d(text_out),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropouts[3])\n",
        "        )\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion_layer = nn.Linear(text_out, rank)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(rank, output_dim)\n",
        "\n",
        "        self.use_softmax = use_softmax\n",
        "\n",
        "    def forward(self, audio_x, video_x, text_x):\n",
        "        # Pass inputs through respective networks\n",
        "        audio_out = self.audio_net(audio_x)\n",
        "        video_out = self.video_net(video_x)\n",
        "        text_out = self.text_net(text_x)\n",
        "\n",
        "                # Concatenate audio and video features\n",
        "        av_concat = torch.cat((audio_out, video_out), dim=1)\n",
        "\n",
        "        # Concatenate audio-video features with text features\n",
        "        avt_concat = torch.cat((av_concat, text_out), dim=1)\n",
        "\n",
        "        # Fusion layer\n",
        "        fusion_out = self.fusion_layer(avt_concat)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(fusion_out)\n",
        "\n",
        "        if self.use_softmax:\n",
        "          output = torch.softmax(output, dim=0)\n",
        "          output = torch.argmax(output, dim=-1)\n",
        "\n",
        "        print (output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "drvfNe5PAIfM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the low-rank multimodal fusion model\n",
        "input_dims = (35, 74, 300)\n",
        "hidden_dims = (50,50,50)\n",
        "text_out = 50\n",
        "dropouts = (0.1, 0.1, 0.1, 0.1)\n",
        "output_dim = 5\n",
        "rank = 5\n",
        "use_softmax = True\n",
        "final_model = LMF(input_dims, hidden_dims, text_out, dropouts, output_dim, rank, use_softmax)\n"
      ],
      "metadata": {
        "id": "476cTJeoc9Iv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function: CrossEntropyLoss for classification tasks\n",
        "Loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: Adam optimizer for training the model parameters\n",
        "optimizer = torch.optim.Adam(list(final_model.parameters())[2:], lr=0.0005, weight_decay=0.01)\n",
        "\n",
        "# Number of epochs for training\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "1tvZsCagc5ar"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(\"EPOCH : \",epoch+1)\n",
        "    # Training\n",
        "    total_train_loss=0.0\n",
        "    num_sequences=0\n",
        "    final_model.train()  # Set the model to training mode\n",
        "    for batch in traindata:\n",
        "        # targets=targets.unsqueeze(1).repeat(1, 50, 1)\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = final_model(*batch[:-1])\n",
        "        loss = Loss(outputs, batch[-1].long())\n",
        "        total_train_loss+=loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "        num_sequences+=1\n",
        "\n",
        "    average_train_loss = total_train_loss / num_sequences\n",
        "    print(\"-------------Training----------------\")\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_train_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    final_model.eval()  # Set the model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    num_sequences=0\n",
        "    val_all_predictions = []\n",
        "    val_all_targets = []\n",
        "    with torch.no_grad():\n",
        "        best_val_loss = np.inf\n",
        "        patience=3\n",
        "        current_patience = patience\n",
        "\n",
        "        for batch in validdata:\n",
        "            val_outputs = final_model(batch[:-1])\n",
        "            val_targets = batch[-1]\n",
        "            val_targets = torch.clamp(val_targets, min=-2, max=2)\n",
        "            val_targets += 2\n",
        "            val_targets = torch.round(val_targets)\n",
        "            val_loss = Loss(val_outputs, val_targets)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Calculate R2 score\n",
        "            val_targets_np = val_targets.cpu().numpy()\n",
        "            val_outputs_np = val_outputs.cpu().numpy()\n",
        "\n",
        "\n",
        "            val_all_predictions.extend(val_outputs_np)\n",
        "            val_all_targets.extend(val_targets_np)\n",
        "            num_sequences+=1\n",
        "\n",
        "    average_val_loss = total_val_loss / num_sequences\n",
        "    accuracy = accuracy_score(val_all_targets, val_all_predictions)\n",
        "\n",
        "    print(\"--------------Validation----------\")\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {average_val_loss:.4f}, Accuracy Score: {accuracy:.4f}')\n",
        "\n",
        "    # if average_val_loss < best_val_loss:\n",
        "    #     best_val_loss=average_val_loss\n",
        "    #     current_patience=patience\n",
        "    # else:\n",
        "    #     current_patience-=1\n",
        "    #     if current_patience==0:\n",
        "    #       print(\"Model performance degarding , Early stopping!!\")\n",
        "    #       break\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n \\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "AZhgL0DBcx8h",
        "outputId": "d6274ada-8c22-479a-d901-c85834e59fbf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH :  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 0,  ..., 1, 4, 3],\n",
            "        [1, 3, 2,  ..., 2, 3, 2],\n",
            "        [1, 1, 0,  ..., 3, 1, 0],\n",
            "        ...,\n",
            "        [2, 2, 2,  ..., 3, 0, 3],\n",
            "        [1, 1, 4,  ..., 2, 3, 0],\n",
            "        [0, 4, 4,  ..., 2, 4, 2]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-dd05ae56bfa2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWtDP2sbyPwp",
        "outputId": "6e2db86d-f41c-4640-903d-d920d98f5131"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000],\n",
              "        [-0.5000],\n",
              "        [-1.0000],\n",
              "        [-2.0000],\n",
              "        [-0.3333],\n",
              "        [ 2.2000],\n",
              "        [-2.2000],\n",
              "        [-1.0000],\n",
              "        [-0.4000],\n",
              "        [ 2.6000],\n",
              "        [-0.2000],\n",
              "        [ 3.0000],\n",
              "        [ 0.6000],\n",
              "        [-1.2000],\n",
              "        [-0.6000],\n",
              "        [-2.0000],\n",
              "        [ 1.6000],\n",
              "        [ 2.0000],\n",
              "        [-2.2000],\n",
              "        [-0.8000],\n",
              "        [ 2.6000],\n",
              "        [ 1.4000],\n",
              "        [ 2.0000],\n",
              "        [-1.4000],\n",
              "        [ 0.2000],\n",
              "        [ 1.8000],\n",
              "        [-1.2000],\n",
              "        [-1.6000],\n",
              "        [ 0.2500],\n",
              "        [-1.2000],\n",
              "        [-1.4000],\n",
              "        [ 1.8000]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(outputs, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9UQHhe0TBFd",
        "outputId": "e1815157-4630-4d43-cf80-d40fc8cfb0cb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0,  ..., 3, 3, 3],\n",
              "        [3, 0, 0,  ..., 3, 3, 0],\n",
              "        [0, 3, 0,  ..., 3, 3, 3],\n",
              "        ...,\n",
              "        [3, 3, 0,  ..., 3, 3, 3],\n",
              "        [0, 3, 3,  ..., 3, 3, 3],\n",
              "        [3, 0, 3,  ..., 3, 3, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader):\n",
        "    \"\"\"\n",
        "    Calculate accuracy for the low-rank multimodal fusion model.\n",
        "\n",
        "    Args:\n",
        "    - model: The low-rank multimodal fusion model.\n",
        "    - dataloader: Dataloader containing the validation data.\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: Accuracy of the model on the validation set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            audio_data, video_data, text_data, labels = batch\n",
        "            predictions = model(audio_data, video_data, text_data)\n",
        "            predictions = torch.argmax(predictions, dim=1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "u8_0BZ4wG8dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_results(train_losses, val_losses, accuracies, fusion_params):\n",
        "    \"\"\"\n",
        "    Display training and validation results along with fusion parameters.\n",
        "\n",
        "    Args:\n",
        "    - train_losses (list): List of training losses for each epoch.\n",
        "    - val_losses (list): List of validation losses for each epoch.\n",
        "    - accuracies (list): List of accuracies for each epoch.\n",
        "    - fusion_params (dict): Dictionary containing fusion parameters.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(accuracies, label='Accuracy', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Display fusion parameters\n",
        "    print(\"Fusion Parameters:\")\n",
        "    for key, value in fusion_params.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "display_results(train_losses, val_losses, accuracies, fusion_params)"
      ],
      "metadata": {
        "id": "C8TKBs3lA8kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfRU7r2EPI_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}