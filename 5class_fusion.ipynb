{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYs7BSKW5Gqk"
      },
      "source": [
        "**Loading** CMU-MOSI **dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f80j9mDfld4U",
        "outputId": "6c982ca4-ebc9-47b9-ee20-1bdfde7a476a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'MultiBench'...\n",
            "remote: Enumerating objects: 6943, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 6943 (delta 72), reused 121 (delta 60), pack-reused 6789\u001b[K\n",
            "Receiving objects: 100% (6943/6943), 51.07 MiB | 12.32 MiB/s, done.\n",
            "Resolving deltas: 100% (4258/4258), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pliang279/MultiBench.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4h7C4XAqNTq",
        "outputId": "5e22ed06-5c3d-4304-f1e0-2a2bfbce86a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MultiBench\n"
          ]
        }
      ],
      "source": [
        "%cd MultiBench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbL-Wu23sEAO",
        "outputId": "4e5060a9-4d0c-492e-cc5e-3de5a6725e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU\n",
            "From (redirected): https://drive.google.com/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU&confirm=t&uuid=557f6c22-3de0-454d-933d-05a74da11f1a\n",
            "To: /content/MultiBench/mosi_raw.pkl\n",
            "100% 357M/357M [00:06<00:00, 55.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!pip install gdown && gdown https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DT1jN1eqsJio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ye24eAGqsP3p"
      },
      "outputs": [],
      "source": [
        "# Import the associated dataloader for affect datasets, which MOSI is a part of.\n",
        "from datasets.affect.get_data import get_dataloader\n",
        "\n",
        "# Create the training, validation, and test-set dataloaders.\n",
        "traindata, validdata, testdata = get_dataloader(\n",
        "    'mosi_raw.pkl', robust_test=False, max_pad=True, data_type='mosi', max_seq_len=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFkqO1W-hI9n"
      },
      "source": [
        "----------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AEkie_f1cqcG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SubnetModel(nn.Module):\n",
        "    def __init__(self, input_size, num_utterances, fc1_size, fc2_size, fc3_size, dropout=0.15):\n",
        "        super(SubnetModel, self).__init__()\n",
        "\n",
        "        # Dropout layer\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(input_size, fc1_size)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)    # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(fc2_size, fc3_size)    # Third fully connected layer\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute mean along the sequence dimension\n",
        "        x = torch.mean(x, dim=1)\n",
        "\n",
        "        # Replace NaN values with zeros\n",
        "        nan_mask = torch.isnan(x)\n",
        "        x[nan_mask] = 0\n",
        "\n",
        "        # Pass through fully connected layers with ReLU activation\n",
        "        fc1_out = self.relu(self.fc1(x))\n",
        "        fc2_out = self.relu(self.fc2(fc1_out))\n",
        "        drop_out = self.drop(fc2_out)\n",
        "\n",
        "        # Final fully connected layer output\n",
        "        fc3_out = self.relu(self.fc3(drop_out))\n",
        "\n",
        "        return fc3_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LcVZJRGndvbw"
      },
      "outputs": [],
      "source": [
        "class TextModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, fc1_size, fc2_size):\n",
        "        super(TextModel, self).__init__()\n",
        "\n",
        "        # LSTM layer (stacked LSTM)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, fc1_size)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)     # Second fully connected layer\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.15)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM layer\n",
        "        lstm_out, (hidden_states, cell_states) = self.lstm(x)\n",
        "\n",
        "        # Pass the hidden states through fully connected layers with ReLU activation\n",
        "        fc1_out = self.relu(self.fc1(hidden_states.squeeze()))\n",
        "        drop_out = self.dropout(fc1_out)\n",
        "        fc2_out = self.relu(self.fc2(drop_out))\n",
        "\n",
        "        return fc2_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "B0LDfUqSga0P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TFN(nn.Module):\n",
        "    def __init__(self, audio_params, video_params, text_params, SIN_params):\n",
        "        super(TFN, self).__init__()\n",
        "\n",
        "        self.output_range = nn.Parameter(torch.FloatTensor([5]), requires_grad=False)\n",
        "        self.output_shift = nn.Parameter(torch.FloatTensor([0]), requires_grad=False)\n",
        "\n",
        "        self.audio_params = audio_params\n",
        "        self.video_params = video_params\n",
        "        self.text_params = text_params\n",
        "\n",
        "        # Unimodal models\n",
        "        self.audio_subnet = SubnetModel(audio_params[0], audio_params[1], audio_params[2], audio_params[3], audio_params[4])\n",
        "        self.video_subnet = SubnetModel(video_params[0], video_params[1], video_params[2], video_params[3], video_params[4])\n",
        "        self.text_subnet = TextModel(text_params[0], text_params[1], text_params[2], text_params[3], text_params[4])\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(((audio_params[2] + 1) * (video_params[2] + 1) * (text_params[3] + 1)), SIN_params[0])\n",
        "        self.fc2 = nn.Linear(SIN_params[0], SIN_params[1])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(SIN_params[1], 5)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x[0].shape[0]\n",
        "\n",
        "        # Unimodal outputs\n",
        "        audio_out = self.audio_subnet(x[0])\n",
        "        video_out = self.video_subnet(x[1])\n",
        "        text_out = self.text_subnet(x[2])\n",
        "\n",
        "        # Adding 1 to increase the dimension value\n",
        "        audio_out = torch.cat((torch.ones(batch_size, 1).to(x[0].device), audio_out), dim=1)\n",
        "        video_out = torch.cat((torch.ones(batch_size, 1).to(x[1].device), video_out), dim=1)\n",
        "        text_out = torch.cat((torch.ones(batch_size, 1).to(x[2].device), text_out), dim=1)\n",
        "\n",
        "        # Tensor fusion operation\n",
        "        fusion_tensor = torch.bmm(audio_out.unsqueeze(2), video_out.unsqueeze(1))\n",
        "        fusion_tensor = fusion_tensor.view(-1, (self.audio_params[2] + 1) * (self.video_params[2] + 1), 1)\n",
        "        fusion_tensor = torch.bmm(fusion_tensor, text_out.unsqueeze(1)).view(batch_size, -1)\n",
        "\n",
        "        # Sentiment inference network (SIN)\n",
        "        fc1_out = self.relu(self.fc1(fusion_tensor))\n",
        "        fc2_out = self.relu(self.fc2(fc1_out))\n",
        "\n",
        "        # Output layer with Sigmoid activation\n",
        "        output = self.softmax(self.output_layer(fc2_out))\n",
        "\n",
        "        # Get output between -3 and +3\n",
        "        # output = output * self.output_range + self.output_shift\n",
        "\n",
        "        output = torch.argmax(output, dim=1)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf3Zk87ghD2O"
      },
      "source": [
        "**-------------------------------------------------------------------------------------------------------------------------------------**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Cv6JFvPMo9Sr"
      },
      "outputs": [],
      "source": [
        "# Audio parameters: (feature_length, max_seq_len, fc1_size, fc2_size, fc3_size)\n",
        "audio_params = (35, 50, 32, 32, 32)\n",
        "\n",
        "# Video parameters: (feature_length, max_seq_len, fc1_size, fc2_size, fc3_size)\n",
        "video_params = (74, 50, 32, 32, 32)\n",
        "\n",
        "# Text parameters: (feature_length, LSTM_hidden_size, num_LSTM_layers, fc1_size, fc2_size)\n",
        "text_params = (300, 128, 1, 128, 128)\n",
        "\n",
        "# Sentiment Inference Network (SIN) parameters: (fc1_size, fc2_size)\n",
        "SIN_params = (128, 128)\n",
        "\n",
        "# Instantiate TFN model with the specified parameters\n",
        "final_model = TFN(audio_params, video_params, text_params, SIN_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "n_C-rC-Itq9G"
      },
      "outputs": [],
      "source": [
        "# Loss function: CrossEntropyLoss for classification tasks\n",
        "Loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: Adam optimizer for training the model parameters\n",
        "optimizer = torch.optim.Adam(list(final_model.parameters())[2:], lr=0.0005, weight_decay=0.01)\n",
        "\n",
        "# Number of epochs for training\n",
        "num_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "PHS0TU9AtcIp",
        "outputId": "0473c855-7059-4651-a266-033f5fe76a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH :  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-0c851df97bdf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
          ]
        }
      ],
      "source": [
        "# Loop through each epoch for training\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(\"EPOCH : \", epoch + 1)\n",
        "\n",
        "    # Training\n",
        "    total_train_loss = 0.0\n",
        "    num_sequences = 0\n",
        "    final_model.train()  # Set the model to training mode\n",
        "\n",
        "    # Iterate over training data batches\n",
        "    for batch in traindata:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = final_model(batch[:-1])\n",
        "        expected_output = batch[-1]\n",
        "\n",
        "        # Clamp the expected output between -2 and 2\n",
        "        expected_output = torch.clamp(expected_output, min=-2, max=2)\n",
        "        expected_output += 2\n",
        "        expected_output = torch.round(expected_output)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = Loss(outputs, expected_output.long())\n",
        "        total_train_loss += loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "        num_sequences += 1\n",
        "\n",
        "    # Calculate average training loss\n",
        "    average_train_loss = total_train_loss / num_sequences\n",
        "    print(\"-------------Training----------------\")\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_train_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    final_model.eval()  # Set the model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    num_sequences = 0\n",
        "    val_all_predictions = []\n",
        "    val_all_targets = []\n",
        "\n",
        "    # Iterate over validation data batches\n",
        "    with torch.no_grad():\n",
        "        for batch in validdata:\n",
        "            val_outputs = final_model(batch[:-1])\n",
        "            val_targets = batch[-1]\n",
        "\n",
        "            # Clamp the validation targets between -2 and 2\n",
        "            val_targets = torch.clamp(val_targets, min=-2, max=2)\n",
        "            val_targets += 2\n",
        "            val_targets = torch.round(val_targets)\n",
        "\n",
        "            # Calculate validation loss\n",
        "            val_loss = Loss(val_outputs, val_targets)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Collect predictions and targets for accuracy calculation\n",
        "            val_targets_np = val_targets.cpu().numpy()\n",
        "            val_outputs_np = val_outputs.cpu().numpy()\n",
        "\n",
        "            val_all_predictions.extend(val_outputs_np)\n",
        "            val_all_targets.extend(val_targets_np)\n",
        "            num_sequences += 1\n",
        "\n",
        "    # Calculate average validation loss and accuracy\n",
        "    average_val_loss = total_val_loss / num_sequences\n",
        "    accuracy = accuracy_score(val_all_targets, val_all_predictions)\n",
        "\n",
        "    print(\"--------------Validation----------\")\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {average_val_loss:.4f}, Accuracy Score: {accuracy:.4f}')\n",
        "    print(\"\\n \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3MlSJ7W5AQa"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK1pV2IPtcIp"
      },
      "outputs": [],
      "source": [
        "# Importing necessary metrics from sklearn\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Set the final model to evaluation mode\n",
        "final_model.eval()\n",
        "\n",
        "# Initialize lists to store predictions and targets\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "# Iterate over test data batches\n",
        "with torch.no_grad():\n",
        "    for batch in testdata:\n",
        "        test_targets = batch[-1]\n",
        "        test_outputs = final_model(batch[:-1])\n",
        "\n",
        "        # Convert predictions and targets to numpy arrays\n",
        "        predictions = test_outputs.numpy().flatten()\n",
        "        targets = test_targets.numpy().flatten()\n",
        "\n",
        "        # Extend the lists with current batch predictions and targets\n",
        "        all_predictions.extend(predictions)\n",
        "        all_targets.extend(targets)\n",
        "\n",
        "    # Calculate Mean Absolute Error and R-squared score\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    r_squared = r2_score(all_targets, all_predictions)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Test MAE: {mae:.4f}, R-squared: {r_squared:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
