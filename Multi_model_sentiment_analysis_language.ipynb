{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYs7BSKW5Gqk"
   },
   "source": [
    "# Loading CMU-MOSI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f80j9mDfld4U",
    "outputId": "1ea8c38c-bb13-400c-af3a-7b141d514223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MultiBench'...\n",
      "remote: Enumerating objects: 6943, done.\u001b[K\n",
      "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 6943 (delta 72), reused 121 (delta 60), pack-reused 6789\u001b[K\n",
      "Receiving objects: 100% (6943/6943), 51.07 MiB | 18.68 MiB/s, done.\n",
      "Resolving deltas: 100% (4258/4258), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pliang279/MultiBench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4h7C4XAqNTq",
    "outputId": "7b8c2fb7-d34a-4655-db47-d379ac31b8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/MultiBench\n"
     ]
    }
   ],
   "source": [
    "%cd MultiBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbL-Wu23sEAO",
    "outputId": "3e9aab26-a171-406c-9de8-546b770f8672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.11/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!pip install gdown && gdown https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DT1jN1eqsJio"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ye24eAGqsP3p"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dynamic module does not define module export function (PyInit__torchtext)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import the associated dataloader for affect datasets, which MOSI is a part of.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maffect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mget_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataloader\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create the training, validation, and test-set dataloaders\u001b[39;00m\n\u001b[1;32m      5\u001b[0m traindata, validdata, testdata \u001b[38;5;241m=\u001b[39m get_dataloader(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mosi_raw.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, robust_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmosi\u001b[39m\u001b[38;5;124m'\u001b[39m, max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m~/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/datasets/affect/get_data.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torchtext/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      8\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/torchtext/_extension.py:61\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "\u001b[0;31mImportError\u001b[0m: dynamic module does not define module export function (PyInit__torchtext)"
     ]
    }
   ],
   "source": [
    "# Import the associated dataloader for affect datasets, which MOSI is a part of.\n",
    "from datasets.affect.get_data import get_dataloader\n",
    "\n",
    "# Create the training, validation, and test-set dataloaders\n",
    "traindata, validdata, testdata = get_dataloader(\n",
    "    '/mosi_raw.pkl', robust_test=False, max_pad=True, data_type='mosi', max_seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-11Hl7VfgwuI",
    "outputId": "764543f1-21a2-4241-fdbb-af7ef4ad9d8b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traindata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtraindata\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(validdata)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(testdata))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(traindata))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(traindata)+len(validdata)+len(testdata))\n",
    "print(len(traindata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYd-nfiZrmnP"
   },
   "source": [
    "The first dimension is often associated with the batch size (32 samples).\n",
    "\n",
    "The second dimension is related to the sequence length (50 time steps).\n",
    "\n",
    "The third dimension represents the number of features or dimensions in each element of the sequence (300 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RKGu0ltAsbhD"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traindata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraindata\u001b[49m:\n\u001b[1;32m      2\u001b[0m   audio,video,text,labels\u001b[38;5;241m=\u001b[39mbatch\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in traindata:\n",
    "  audio,video,text,labels=batch\n",
    "\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPHHBQUKsXpq"
   },
   "source": [
    "each word is stored as GloVe embedding\n",
    "\n",
    "basically it means i have a batch size of 32 objects , each object has 50 words and each word is represented as vector of 300 features (GloVe embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ma2NZ0Fx2IMG",
    "outputId": "72f82cee-ee01-4b5c-afa1-7fa645663348"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traindata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mtraindata\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(traindata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qicCkSMr4lLh"
   },
   "source": [
    "# Extracting the text and labels as torch.tensor objects and storing as torch dataloader to give model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K4e-8flA2X4j"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader,ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NkBV0arABHO9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traindata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m all_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraindata\u001b[49m:\n\u001b[1;32m      5\u001b[0m     text, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m#text and labels are the last two components\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     all_text\u001b[38;5;241m.\u001b[39mappend(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
     ]
    }
   ],
   "source": [
    "all_text = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in traindata:\n",
    "    text, labels = batch[-2:]  #text and labels are the last two components\n",
    "    all_text.append(text)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_text = torch.cat(all_text, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Create a new DataLoader with all text and labels\n",
    "new_dataset = TensorDataset(all_text, all_labels)\n",
    "train_text_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tifw1Keqjeak",
    "outputId": "f19bbbe1-bd05-45a8-8dc1-7c283efd209b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0pc1rgc4lJl",
    "outputId": "fb31247c-90c4-4ca6-a4de-bbeb56f19afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for i in train_text_dataloader:\n",
    "  a,b=i\n",
    "  print(b.shape)\n",
    "  print(type(b))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH1i_lePCjzi"
   },
   "outputs": [],
   "source": [
    "all_text = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in validdata:\n",
    "    text, labels = batch[-2:]  # text and labels are the last two components\n",
    "    all_text.append(text)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_text = torch.cat(all_text, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Create a new DataLoader with all text and labels\n",
    "new_dataset = TensorDataset(all_text, all_labels)\n",
    "val_text_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE00ZSpdCriA"
   },
   "outputs": [],
   "source": [
    "all_text = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in testdata:\n",
    "    text, labels = batch[-2:]  # Assuming text and labels are the last two components\n",
    "    all_text.append(text)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_text = torch.cat(all_text, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Create a new DataLoader with all text and labels\n",
    "new_dataset = TensorDataset(all_text, all_labels)\n",
    "test_text_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ixDS9q2V-PT"
   },
   "outputs": [],
   "source": [
    "for a,b in test_text_dataloader:\n",
    "  print(b)\n",
    "  b=b.unsqueeze(1).repeat(1, 50, 1)\n",
    "  print(\"-------------\")\n",
    "  print(b)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbZ1eCSm47pE"
   },
   "source": [
    "# Creating text model (unimodel) architecture from paper for sentiment regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEkie_f1cqcG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers,fc1_size, fc2_size,si_size):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "        self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "        # LSTM layer (stacked LSTM)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "\n",
    "        #fc1 gets hidden_size dimension values as input\n",
    "        self.fc1 = nn.Linear(hidden_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.si1=nn.Linear(fc2_size,si_size)\n",
    "        self.si2=nn.Linear(si_size,si_size)\n",
    "\n",
    "        self.dropout=nn.Dropout(p=0.15)\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(fc2_size, 1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1, x.size(-1), x.size(-2))\n",
    "\n",
    "        # LSTM layer\n",
    "\n",
    "        lstm_out, (hidden_states, cell_states) = self.lstm(x)\n",
    "\n",
    "        fc1_out = self.relu(self.fc1(hidden_states.squeeze()))\n",
    "        drop = self.dropout(fc1_out)\n",
    "        fc2_out = self.relu(self.fc2(drop))\n",
    "\n",
    "\n",
    "        si1=self.relu(self.si1(fc2_out))\n",
    "        si2=self.relu(self.si2(si1))\n",
    "\n",
    "        # Output layer with Sigmoid activation\n",
    "        output = self.sigmoid(self.output_layer(si2))\n",
    "        #get output between -3 and +3\n",
    "        output=output*self.output_range+self.output_shift\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPQF1yjpqYs_"
   },
   "source": [
    "## **Hidden size parameter in a LSTM**\n",
    "\n",
    "1) Independence: In principle, the hidden size does not need to be directly related to the input size. You could have a large hidden size with a small input size or vice versa. The hidden size determines the capacity of the LSTM to capture and store information across time steps, while the input size defines the dimensionality of the input data at each time step.\n",
    "\n",
    "2) Task Complexity: The choice of the hidden size is often influenced by the complexity of the task you are working on. More complex tasks may benefit from a larger hidden size as it provides the model with more capacity to learn intricate patterns in the data.\n",
    "\n",
    "3) Computational Resources: Larger hidden sizes come with increased computational requirements. Training models with larger hidden sizes may take more time and resources. Therefore, the choice of hidden size can also depend on the available computational resources.\n",
    "\n",
    "\n",
    "\n",
    "## **num_layers parameter in LSTM:**\n",
    "\n",
    "Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3-POp9nyEPj"
   },
   "source": [
    "## **Hidden state of a LSTM unit:**\n",
    "\n",
    "Let's break down the components of the hidden state:\n",
    "\n",
    "1) Memory of Previous Information:\n",
    "\n",
    "The hidden state encapsulates information from previous time steps in a sequence.\n",
    "It acts as a form of memory that allows the LSTM to retain information over long distances in the sequence, mitigating the vanishing gradient problem encountered in simpler RNN architectures.\n",
    "\n",
    "2) Capture of Relevant Patterns:\n",
    "\n",
    "The LSTM hidden state is designed to capture and store relevant patterns, dependencies, and context within the sequence.\n",
    "It selectively updates and forgets information through its gating mechanisms (input gate, forget gate, and output gate), allowing it to focus on important elements and discard less relevant information.\n",
    "\n",
    "3) Interaction with Current Input:\n",
    "\n",
    "The hidden state influences the processing of the current input at the current time step.\n",
    "It combines information from the previous hidden state with the information from the current input to produce an updated hidden state for the current time step.\n",
    "\n",
    "4) Encoded Representation:\n",
    "\n",
    "The hidden state serves as an encoded representation of the information learned from the entire history of the sequence processed up to the current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHNkCKns9zas",
    "outputId": "7f017956-2207-408e-b19b-9c72d0016895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (lstm): LSTM(300, 128, batch_first=True)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (si1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (si2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.15, inplace=False)\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "\n",
    "#LSTM parameters\n",
    "input_size = 300 # since each word is in GloVe embedding format\n",
    "hidden_size = 128 # hyperparameter for each LSTM layer. refers to the dimension of the hidden state value\n",
    "num_layers=1\n",
    "\n",
    "# following are values defined in paper\n",
    "fc1_size = 128\n",
    "fc2_size = 128\n",
    "si_size=128\n",
    "\n",
    "model = LanguageModel(input_size, hidden_size,num_layers, fc1_size, fc2_size,si_size)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcBofb2LX88_"
   },
   "source": [
    "# Training the text model (unimodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrPZ9dasViKa"
   },
   "outputs": [],
   "source": [
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters())[2:],lr=0.00006)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCwVEkioAK9V"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arTzgDxLVYjV",
    "outputId": "67dc5997-7387-4eb2-e307-ea16d0e6331a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH :  1\n",
      "-------------Training----------------\n",
      "Epoch [1/150], Loss: 2.3158\n",
      "--------------Validation----------\n",
      "Epoch [1/150], Validation Loss: 2.8059, R2 Score: -0.0763\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  2\n",
      "-------------Training----------------\n",
      "Epoch [2/150], Loss: 2.3659\n",
      "--------------Validation----------\n",
      "Epoch [2/150], Validation Loss: 2.7682, R2 Score: -0.0555\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  3\n",
      "-------------Training----------------\n",
      "Epoch [3/150], Loss: 2.3208\n",
      "--------------Validation----------\n",
      "Epoch [3/150], Validation Loss: 2.6769, R2 Score: -0.0383\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  4\n",
      "-------------Training----------------\n",
      "Epoch [4/150], Loss: 2.3170\n",
      "--------------Validation----------\n",
      "Epoch [4/150], Validation Loss: 2.7116, R2 Score: -0.0275\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  5\n",
      "-------------Training----------------\n",
      "Epoch [5/150], Loss: 2.3081\n",
      "--------------Validation----------\n",
      "Epoch [5/150], Validation Loss: 2.6107, R2 Score: -0.0259\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  6\n",
      "-------------Training----------------\n",
      "Epoch [6/150], Loss: 2.2874\n",
      "--------------Validation----------\n",
      "Epoch [6/150], Validation Loss: 2.6728, R2 Score: -0.0226\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  7\n",
      "-------------Training----------------\n",
      "Epoch [7/150], Loss: 2.2651\n",
      "--------------Validation----------\n",
      "Epoch [7/150], Validation Loss: 2.6283, R2 Score: -0.0184\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  8\n",
      "-------------Training----------------\n",
      "Epoch [8/150], Loss: 2.2322\n",
      "--------------Validation----------\n",
      "Epoch [8/150], Validation Loss: 2.6183, R2 Score: -0.0133\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  9\n",
      "-------------Training----------------\n",
      "Epoch [9/150], Loss: 2.2642\n",
      "--------------Validation----------\n",
      "Epoch [9/150], Validation Loss: 2.6524, R2 Score: -0.0149\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  10\n",
      "-------------Training----------------\n",
      "Epoch [10/150], Loss: 2.2633\n",
      "--------------Validation----------\n",
      "Epoch [10/150], Validation Loss: 2.6182, R2 Score: -0.0149\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  11\n",
      "-------------Training----------------\n",
      "Epoch [11/150], Loss: 2.2473\n",
      "--------------Validation----------\n",
      "Epoch [11/150], Validation Loss: 2.6540, R2 Score: -0.0158\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  12\n",
      "-------------Training----------------\n",
      "Epoch [12/150], Loss: 2.3015\n",
      "--------------Validation----------\n",
      "Epoch [12/150], Validation Loss: 2.6610, R2 Score: -0.0154\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  13\n",
      "-------------Training----------------\n",
      "Epoch [13/150], Loss: 2.2953\n",
      "--------------Validation----------\n",
      "Epoch [13/150], Validation Loss: 2.6521, R2 Score: -0.0165\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  14\n",
      "-------------Training----------------\n",
      "Epoch [14/150], Loss: 2.2826\n",
      "--------------Validation----------\n",
      "Epoch [14/150], Validation Loss: 2.6583, R2 Score: -0.0163\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  15\n",
      "-------------Training----------------\n",
      "Epoch [15/150], Loss: 2.2655\n",
      "--------------Validation----------\n",
      "Epoch [15/150], Validation Loss: 2.6578, R2 Score: -0.0135\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  16\n",
      "-------------Training----------------\n",
      "Epoch [16/150], Loss: 2.2869\n",
      "--------------Validation----------\n",
      "Epoch [16/150], Validation Loss: 2.6133, R2 Score: -0.0150\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  17\n",
      "-------------Training----------------\n",
      "Epoch [17/150], Loss: 2.2614\n",
      "--------------Validation----------\n",
      "Epoch [17/150], Validation Loss: 2.6319, R2 Score: -0.0135\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  18\n",
      "-------------Training----------------\n",
      "Epoch [18/150], Loss: 2.2362\n",
      "--------------Validation----------\n",
      "Epoch [18/150], Validation Loss: 2.6367, R2 Score: -0.0151\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  19\n",
      "-------------Training----------------\n",
      "Epoch [19/150], Loss: 2.2712\n",
      "--------------Validation----------\n",
      "Epoch [19/150], Validation Loss: 2.6482, R2 Score: -0.0150\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  20\n",
      "-------------Training----------------\n",
      "Epoch [20/150], Loss: 2.2442\n",
      "--------------Validation----------\n",
      "Epoch [20/150], Validation Loss: 2.6113, R2 Score: -0.0163\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  21\n",
      "-------------Training----------------\n",
      "Epoch [21/150], Loss: 2.2853\n",
      "--------------Validation----------\n",
      "Epoch [21/150], Validation Loss: 2.6296, R2 Score: -0.0159\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  22\n",
      "-------------Training----------------\n",
      "Epoch [22/150], Loss: 2.2782\n",
      "--------------Validation----------\n",
      "Epoch [22/150], Validation Loss: 2.6222, R2 Score: -0.0160\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  23\n",
      "-------------Training----------------\n",
      "Epoch [23/150], Loss: 2.2584\n",
      "--------------Validation----------\n",
      "Epoch [23/150], Validation Loss: 2.6232, R2 Score: -0.0133\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  24\n",
      "-------------Training----------------\n",
      "Epoch [24/150], Loss: 2.2404\n",
      "--------------Validation----------\n",
      "Epoch [24/150], Validation Loss: 2.6234, R2 Score: -0.0152\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  25\n",
      "-------------Training----------------\n",
      "Epoch [25/150], Loss: 2.2666\n",
      "--------------Validation----------\n",
      "Epoch [25/150], Validation Loss: 2.6497, R2 Score: -0.0155\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  26\n",
      "-------------Training----------------\n",
      "Epoch [26/150], Loss: 2.2775\n",
      "--------------Validation----------\n",
      "Epoch [26/150], Validation Loss: 2.6585, R2 Score: -0.0195\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  27\n",
      "-------------Training----------------\n",
      "Epoch [27/150], Loss: 2.3101\n",
      "--------------Validation----------\n",
      "Epoch [27/150], Validation Loss: 2.6144, R2 Score: -0.0107\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  28\n",
      "-------------Training----------------\n",
      "Epoch [28/150], Loss: 2.2279\n",
      "--------------Validation----------\n",
      "Epoch [28/150], Validation Loss: 2.6500, R2 Score: -0.0149\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  29\n",
      "-------------Training----------------\n",
      "Epoch [29/150], Loss: 2.2868\n",
      "--------------Validation----------\n",
      "Epoch [29/150], Validation Loss: 2.5937, R2 Score: -0.0171\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  30\n",
      "-------------Training----------------\n",
      "Epoch [30/150], Loss: 2.3056\n",
      "--------------Validation----------\n",
      "Epoch [30/150], Validation Loss: 2.6073, R2 Score: -0.0143\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  31\n",
      "-------------Training----------------\n",
      "Epoch [31/150], Loss: 2.2521\n",
      "--------------Validation----------\n",
      "Epoch [31/150], Validation Loss: 2.6437, R2 Score: -0.0149\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  32\n",
      "-------------Training----------------\n",
      "Epoch [32/150], Loss: 2.2855\n",
      "--------------Validation----------\n",
      "Epoch [32/150], Validation Loss: 2.6522, R2 Score: -0.0175\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  33\n",
      "-------------Training----------------\n",
      "Epoch [33/150], Loss: 2.2389\n",
      "--------------Validation----------\n",
      "Epoch [33/150], Validation Loss: 2.6339, R2 Score: -0.0151\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  34\n",
      "-------------Training----------------\n",
      "Epoch [34/150], Loss: 2.2296\n",
      "--------------Validation----------\n",
      "Epoch [34/150], Validation Loss: 2.6590, R2 Score: -0.0143\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  35\n",
      "-------------Training----------------\n",
      "Epoch [35/150], Loss: 2.2407\n",
      "--------------Validation----------\n",
      "Epoch [35/150], Validation Loss: 2.5904, R2 Score: -0.0135\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  36\n",
      "-------------Training----------------\n",
      "Epoch [36/150], Loss: 2.2424\n",
      "--------------Validation----------\n",
      "Epoch [36/150], Validation Loss: 2.6361, R2 Score: -0.0120\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  37\n",
      "-------------Training----------------\n",
      "Epoch [37/150], Loss: 2.2446\n",
      "--------------Validation----------\n",
      "Epoch [37/150], Validation Loss: 2.6249, R2 Score: -0.0130\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  38\n",
      "-------------Training----------------\n",
      "Epoch [38/150], Loss: 2.2153\n",
      "--------------Validation----------\n",
      "Epoch [38/150], Validation Loss: 2.5953, R2 Score: -0.0109\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  39\n",
      "-------------Training----------------\n",
      "Epoch [39/150], Loss: 2.2411\n",
      "--------------Validation----------\n",
      "Epoch [39/150], Validation Loss: 2.6459, R2 Score: -0.0146\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  40\n",
      "-------------Training----------------\n",
      "Epoch [40/150], Loss: 2.2695\n",
      "--------------Validation----------\n",
      "Epoch [40/150], Validation Loss: 2.5982, R2 Score: -0.0134\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  41\n",
      "-------------Training----------------\n",
      "Epoch [41/150], Loss: 2.2511\n",
      "--------------Validation----------\n",
      "Epoch [41/150], Validation Loss: 2.6106, R2 Score: -0.0097\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  42\n",
      "-------------Training----------------\n",
      "Epoch [42/150], Loss: 2.2194\n",
      "--------------Validation----------\n",
      "Epoch [42/150], Validation Loss: 2.6018, R2 Score: -0.0119\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  43\n",
      "-------------Training----------------\n",
      "Epoch [43/150], Loss: 2.2444\n",
      "--------------Validation----------\n",
      "Epoch [43/150], Validation Loss: 2.5983, R2 Score: -0.0087\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  44\n",
      "-------------Training----------------\n",
      "Epoch [44/150], Loss: 2.2508\n",
      "--------------Validation----------\n",
      "Epoch [44/150], Validation Loss: 2.6376, R2 Score: -0.0246\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  45\n",
      "-------------Training----------------\n",
      "Epoch [45/150], Loss: 2.2230\n",
      "--------------Validation----------\n",
      "Epoch [45/150], Validation Loss: 2.6241, R2 Score: -0.0078\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  46\n",
      "-------------Training----------------\n",
      "Epoch [46/150], Loss: 2.2141\n",
      "--------------Validation----------\n",
      "Epoch [46/150], Validation Loss: 2.5843, R2 Score: -0.0134\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  47\n",
      "-------------Training----------------\n",
      "Epoch [47/150], Loss: 2.2035\n",
      "--------------Validation----------\n",
      "Epoch [47/150], Validation Loss: 2.7039, R2 Score: -0.0398\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  48\n",
      "-------------Training----------------\n",
      "Epoch [48/150], Loss: 2.2149\n",
      "--------------Validation----------\n",
      "Epoch [48/150], Validation Loss: 2.6079, R2 Score: 0.0018\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  49\n",
      "-------------Training----------------\n",
      "Epoch [49/150], Loss: 2.1306\n",
      "--------------Validation----------\n",
      "Epoch [49/150], Validation Loss: 2.5678, R2 Score: 0.0045\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  50\n",
      "-------------Training----------------\n",
      "Epoch [50/150], Loss: 2.1266\n",
      "--------------Validation----------\n",
      "Epoch [50/150], Validation Loss: 2.6846, R2 Score: -0.0204\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  51\n",
      "-------------Training----------------\n",
      "Epoch [51/150], Loss: 2.0928\n",
      "--------------Validation----------\n",
      "Epoch [51/150], Validation Loss: 2.5799, R2 Score: 0.0132\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  52\n",
      "-------------Training----------------\n",
      "Epoch [52/150], Loss: 2.0602\n",
      "--------------Validation----------\n",
      "Epoch [52/150], Validation Loss: 2.5958, R2 Score: 0.0007\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  53\n",
      "-------------Training----------------\n",
      "Epoch [53/150], Loss: 1.8976\n",
      "--------------Validation----------\n",
      "Epoch [53/150], Validation Loss: 2.5738, R2 Score: -0.0050\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  54\n",
      "-------------Training----------------\n",
      "Epoch [54/150], Loss: 1.9178\n",
      "--------------Validation----------\n",
      "Epoch [54/150], Validation Loss: 2.4012, R2 Score: 0.0841\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  55\n",
      "-------------Training----------------\n",
      "Epoch [55/150], Loss: 1.7280\n",
      "--------------Validation----------\n",
      "Epoch [55/150], Validation Loss: 2.3250, R2 Score: 0.1087\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  56\n",
      "-------------Training----------------\n",
      "Epoch [56/150], Loss: 1.6995\n",
      "--------------Validation----------\n",
      "Epoch [56/150], Validation Loss: 2.1658, R2 Score: 0.1552\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  57\n",
      "-------------Training----------------\n",
      "Epoch [57/150], Loss: 1.6651\n",
      "--------------Validation----------\n",
      "Epoch [57/150], Validation Loss: 2.1582, R2 Score: 0.1654\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  58\n",
      "-------------Training----------------\n",
      "Epoch [58/150], Loss: 1.5706\n",
      "--------------Validation----------\n",
      "Epoch [58/150], Validation Loss: 2.0693, R2 Score: 0.2001\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  59\n",
      "-------------Training----------------\n",
      "Epoch [59/150], Loss: 1.4862\n",
      "--------------Validation----------\n",
      "Epoch [59/150], Validation Loss: 2.0463, R2 Score: 0.2109\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  60\n",
      "-------------Training----------------\n",
      "Epoch [60/150], Loss: 1.4398\n",
      "--------------Validation----------\n",
      "Epoch [60/150], Validation Loss: 2.1508, R2 Score: 0.1597\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  61\n",
      "-------------Training----------------\n",
      "Epoch [61/150], Loss: 1.4135\n",
      "--------------Validation----------\n",
      "Epoch [61/150], Validation Loss: 1.9543, R2 Score: 0.2382\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  62\n",
      "-------------Training----------------\n",
      "Epoch [62/150], Loss: 1.3596\n",
      "--------------Validation----------\n",
      "Epoch [62/150], Validation Loss: 2.0603, R2 Score: 0.2058\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  63\n",
      "-------------Training----------------\n",
      "Epoch [63/150], Loss: 1.3454\n",
      "--------------Validation----------\n",
      "Epoch [63/150], Validation Loss: 2.0810, R2 Score: 0.1986\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  64\n",
      "-------------Training----------------\n",
      "Epoch [64/150], Loss: 1.4088\n",
      "--------------Validation----------\n",
      "Epoch [64/150], Validation Loss: 2.0494, R2 Score: 0.2107\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  65\n",
      "-------------Training----------------\n",
      "Epoch [65/150], Loss: 1.2894\n",
      "--------------Validation----------\n",
      "Epoch [65/150], Validation Loss: 1.8797, R2 Score: 0.2578\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  66\n",
      "-------------Training----------------\n",
      "Epoch [66/150], Loss: 1.3336\n",
      "--------------Validation----------\n",
      "Epoch [66/150], Validation Loss: 1.9098, R2 Score: 0.2545\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  67\n",
      "-------------Training----------------\n",
      "Epoch [67/150], Loss: 1.2340\n",
      "--------------Validation----------\n",
      "Epoch [67/150], Validation Loss: 1.9638, R2 Score: 0.2371\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  68\n",
      "-------------Training----------------\n",
      "Epoch [68/150], Loss: 1.1875\n",
      "--------------Validation----------\n",
      "Epoch [68/150], Validation Loss: 1.8597, R2 Score: 0.2849\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  69\n",
      "-------------Training----------------\n",
      "Epoch [69/150], Loss: 1.2228\n",
      "--------------Validation----------\n",
      "Epoch [69/150], Validation Loss: 1.8501, R2 Score: 0.2807\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  70\n",
      "-------------Training----------------\n",
      "Epoch [70/150], Loss: 1.2149\n",
      "--------------Validation----------\n",
      "Epoch [70/150], Validation Loss: 1.8135, R2 Score: 0.2967\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  71\n",
      "-------------Training----------------\n",
      "Epoch [71/150], Loss: 1.2514\n",
      "--------------Validation----------\n",
      "Epoch [71/150], Validation Loss: 1.8230, R2 Score: 0.2920\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  72\n",
      "-------------Training----------------\n",
      "Epoch [72/150], Loss: 1.1822\n",
      "--------------Validation----------\n",
      "Epoch [72/150], Validation Loss: 1.8813, R2 Score: 0.2872\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  73\n",
      "-------------Training----------------\n",
      "Epoch [73/150], Loss: 1.1256\n",
      "--------------Validation----------\n",
      "Epoch [73/150], Validation Loss: 1.9125, R2 Score: 0.2588\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  74\n",
      "-------------Training----------------\n",
      "Epoch [74/150], Loss: 1.0925\n",
      "--------------Validation----------\n",
      "Epoch [74/150], Validation Loss: 1.9441, R2 Score: 0.2461\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  75\n",
      "-------------Training----------------\n",
      "Epoch [75/150], Loss: 1.1310\n",
      "--------------Validation----------\n",
      "Epoch [75/150], Validation Loss: 1.8025, R2 Score: 0.2937\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  76\n",
      "-------------Training----------------\n",
      "Epoch [76/150], Loss: 1.0815\n",
      "--------------Validation----------\n",
      "Epoch [76/150], Validation Loss: 1.8782, R2 Score: 0.2839\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  77\n",
      "-------------Training----------------\n",
      "Epoch [77/150], Loss: 1.0900\n",
      "--------------Validation----------\n",
      "Epoch [77/150], Validation Loss: 1.8909, R2 Score: 0.2658\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  78\n",
      "-------------Training----------------\n",
      "Epoch [78/150], Loss: 1.0213\n",
      "--------------Validation----------\n",
      "Epoch [78/150], Validation Loss: 1.8711, R2 Score: 0.2813\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  79\n",
      "-------------Training----------------\n",
      "Epoch [79/150], Loss: 1.0369\n",
      "--------------Validation----------\n",
      "Epoch [79/150], Validation Loss: 1.8193, R2 Score: 0.3007\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  80\n",
      "-------------Training----------------\n",
      "Epoch [80/150], Loss: 1.0056\n",
      "--------------Validation----------\n",
      "Epoch [80/150], Validation Loss: 1.8912, R2 Score: 0.2855\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  81\n",
      "-------------Training----------------\n",
      "Epoch [81/150], Loss: 1.1198\n",
      "--------------Validation----------\n",
      "Epoch [81/150], Validation Loss: 1.8341, R2 Score: 0.2871\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  82\n",
      "-------------Training----------------\n",
      "Epoch [82/150], Loss: 0.9779\n",
      "--------------Validation----------\n",
      "Epoch [82/150], Validation Loss: 1.8029, R2 Score: 0.3029\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  83\n",
      "-------------Training----------------\n",
      "Epoch [83/150], Loss: 0.9562\n",
      "--------------Validation----------\n",
      "Epoch [83/150], Validation Loss: 1.8298, R2 Score: 0.2895\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  84\n",
      "-------------Training----------------\n",
      "Epoch [84/150], Loss: 0.9976\n",
      "--------------Validation----------\n",
      "Epoch [84/150], Validation Loss: 1.8317, R2 Score: 0.2891\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  85\n",
      "-------------Training----------------\n",
      "Epoch [85/150], Loss: 1.0765\n",
      "--------------Validation----------\n",
      "Epoch [85/150], Validation Loss: 1.8250, R2 Score: 0.3046\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  86\n",
      "-------------Training----------------\n",
      "Epoch [86/150], Loss: 0.9524\n",
      "--------------Validation----------\n",
      "Epoch [86/150], Validation Loss: 1.8492, R2 Score: 0.2887\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  87\n",
      "-------------Training----------------\n",
      "Epoch [87/150], Loss: 0.9237\n",
      "--------------Validation----------\n",
      "Epoch [87/150], Validation Loss: 1.7026, R2 Score: 0.3294\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  88\n",
      "-------------Training----------------\n",
      "Epoch [88/150], Loss: 0.9497\n",
      "--------------Validation----------\n",
      "Epoch [88/150], Validation Loss: 1.7448, R2 Score: 0.3292\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  89\n",
      "-------------Training----------------\n",
      "Epoch [89/150], Loss: 0.9421\n",
      "--------------Validation----------\n",
      "Epoch [89/150], Validation Loss: 1.7145, R2 Score: 0.3191\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  90\n",
      "-------------Training----------------\n",
      "Epoch [90/150], Loss: 0.9105\n",
      "--------------Validation----------\n",
      "Epoch [90/150], Validation Loss: 1.8751, R2 Score: 0.2847\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  91\n",
      "-------------Training----------------\n",
      "Epoch [91/150], Loss: 0.8901\n",
      "--------------Validation----------\n",
      "Epoch [91/150], Validation Loss: 1.7762, R2 Score: 0.3120\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  92\n",
      "-------------Training----------------\n",
      "Epoch [92/150], Loss: 0.8832\n",
      "--------------Validation----------\n",
      "Epoch [92/150], Validation Loss: 1.7749, R2 Score: 0.3093\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  93\n",
      "-------------Training----------------\n",
      "Epoch [93/150], Loss: 0.8654\n",
      "--------------Validation----------\n",
      "Epoch [93/150], Validation Loss: 1.8728, R2 Score: 0.2926\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  94\n",
      "-------------Training----------------\n",
      "Epoch [94/150], Loss: 0.8981\n",
      "--------------Validation----------\n",
      "Epoch [94/150], Validation Loss: 1.7350, R2 Score: 0.3268\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  95\n",
      "-------------Training----------------\n",
      "Epoch [95/150], Loss: 0.9572\n",
      "--------------Validation----------\n",
      "Epoch [95/150], Validation Loss: 1.7399, R2 Score: 0.3202\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  96\n",
      "-------------Training----------------\n",
      "Epoch [96/150], Loss: 0.8391\n",
      "--------------Validation----------\n",
      "Epoch [96/150], Validation Loss: 1.9074, R2 Score: 0.2525\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  97\n",
      "-------------Training----------------\n",
      "Epoch [97/150], Loss: 0.8181\n",
      "--------------Validation----------\n",
      "Epoch [97/150], Validation Loss: 1.8468, R2 Score: 0.2920\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  98\n",
      "-------------Training----------------\n",
      "Epoch [98/150], Loss: 0.8134\n",
      "--------------Validation----------\n",
      "Epoch [98/150], Validation Loss: 1.8173, R2 Score: 0.3072\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  99\n",
      "-------------Training----------------\n",
      "Epoch [99/150], Loss: 0.7987\n",
      "--------------Validation----------\n",
      "Epoch [99/150], Validation Loss: 1.7454, R2 Score: 0.3207\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  100\n",
      "-------------Training----------------\n",
      "Epoch [100/150], Loss: 0.8241\n",
      "--------------Validation----------\n",
      "Epoch [100/150], Validation Loss: 1.7735, R2 Score: 0.3037\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  101\n",
      "-------------Training----------------\n",
      "Epoch [101/150], Loss: 0.8138\n",
      "--------------Validation----------\n",
      "Epoch [101/150], Validation Loss: 1.7630, R2 Score: 0.3209\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  102\n",
      "-------------Training----------------\n",
      "Epoch [102/150], Loss: 0.7816\n",
      "--------------Validation----------\n",
      "Epoch [102/150], Validation Loss: 1.7558, R2 Score: 0.3175\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  103\n",
      "-------------Training----------------\n",
      "Epoch [103/150], Loss: 0.8118\n",
      "--------------Validation----------\n",
      "Epoch [103/150], Validation Loss: 1.8247, R2 Score: 0.2989\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  104\n",
      "-------------Training----------------\n",
      "Epoch [104/150], Loss: 0.8331\n",
      "--------------Validation----------\n",
      "Epoch [104/150], Validation Loss: 1.7984, R2 Score: 0.3007\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  105\n",
      "-------------Training----------------\n",
      "Epoch [105/150], Loss: 0.8458\n",
      "--------------Validation----------\n",
      "Epoch [105/150], Validation Loss: 1.8967, R2 Score: 0.2822\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  106\n",
      "-------------Training----------------\n",
      "Epoch [106/150], Loss: 0.7412\n",
      "--------------Validation----------\n",
      "Epoch [106/150], Validation Loss: 1.7731, R2 Score: 0.3169\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  107\n",
      "-------------Training----------------\n",
      "Epoch [107/150], Loss: 0.7390\n",
      "--------------Validation----------\n",
      "Epoch [107/150], Validation Loss: 1.8424, R2 Score: 0.3024\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  108\n",
      "-------------Training----------------\n",
      "Epoch [108/150], Loss: 0.7624\n",
      "--------------Validation----------\n",
      "Epoch [108/150], Validation Loss: 1.7233, R2 Score: 0.3257\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  109\n",
      "-------------Training----------------\n",
      "Epoch [109/150], Loss: 0.7227\n",
      "--------------Validation----------\n",
      "Epoch [109/150], Validation Loss: 1.8420, R2 Score: 0.2776\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  110\n",
      "-------------Training----------------\n",
      "Epoch [110/150], Loss: 0.8479\n",
      "--------------Validation----------\n",
      "Epoch [110/150], Validation Loss: 1.7228, R2 Score: 0.3383\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  111\n",
      "-------------Training----------------\n",
      "Epoch [111/150], Loss: 0.7433\n",
      "--------------Validation----------\n",
      "Epoch [111/150], Validation Loss: 1.8009, R2 Score: 0.3135\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  112\n",
      "-------------Training----------------\n",
      "Epoch [112/150], Loss: 0.7103\n",
      "--------------Validation----------\n",
      "Epoch [112/150], Validation Loss: 1.7318, R2 Score: 0.3372\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  113\n",
      "-------------Training----------------\n",
      "Epoch [113/150], Loss: 0.6842\n",
      "--------------Validation----------\n",
      "Epoch [113/150], Validation Loss: 1.7355, R2 Score: 0.3192\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  114\n",
      "-------------Training----------------\n",
      "Epoch [114/150], Loss: 0.6856\n",
      "--------------Validation----------\n",
      "Epoch [114/150], Validation Loss: 1.7668, R2 Score: 0.3241\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  115\n",
      "-------------Training----------------\n",
      "Epoch [115/150], Loss: 0.6991\n",
      "--------------Validation----------\n",
      "Epoch [115/150], Validation Loss: 1.7578, R2 Score: 0.3139\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  116\n",
      "-------------Training----------------\n",
      "Epoch [116/150], Loss: 0.6725\n",
      "--------------Validation----------\n",
      "Epoch [116/150], Validation Loss: 1.8070, R2 Score: 0.2989\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  117\n",
      "-------------Training----------------\n",
      "Epoch [117/150], Loss: 0.6705\n",
      "--------------Validation----------\n",
      "Epoch [117/150], Validation Loss: 1.7575, R2 Score: 0.3328\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  118\n",
      "-------------Training----------------\n",
      "Epoch [118/150], Loss: 0.7748\n",
      "--------------Validation----------\n",
      "Epoch [118/150], Validation Loss: 1.7976, R2 Score: 0.2988\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  119\n",
      "-------------Training----------------\n",
      "Epoch [119/150], Loss: 0.6717\n",
      "--------------Validation----------\n",
      "Epoch [119/150], Validation Loss: 1.9039, R2 Score: 0.2610\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  120\n",
      "-------------Training----------------\n",
      "Epoch [120/150], Loss: 0.6899\n",
      "--------------Validation----------\n",
      "Epoch [120/150], Validation Loss: 1.8407, R2 Score: 0.2953\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  121\n",
      "-------------Training----------------\n",
      "Epoch [121/150], Loss: 0.6768\n",
      "--------------Validation----------\n",
      "Epoch [121/150], Validation Loss: 1.7781, R2 Score: 0.3214\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  122\n",
      "-------------Training----------------\n",
      "Epoch [122/150], Loss: 0.6526\n",
      "--------------Validation----------\n",
      "Epoch [122/150], Validation Loss: 1.7738, R2 Score: 0.3106\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  123\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(\"EPOCH : \",epoch+1)\n",
    "    # Training\n",
    "    total_train_loss=0.0\n",
    "    num_sequences=0\n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, targets in train_text_dataloader:\n",
    "        # targets=targets.unsqueeze(1).repeat(1, 50, 1)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = Loss(outputs, targets)\n",
    "        total_train_loss+=loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        num_sequences+=1\n",
    "\n",
    "    average_train_loss = total_train_loss / num_sequences\n",
    "    print(\"-------------Training----------------\")\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_train_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0.0\n",
    "        num_sequences=0\n",
    "        val_all_predictions = []\n",
    "        val_all_targets = []\n",
    "        for val_inputs, val_targets in val_text_dataloader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = Loss(val_outputs, val_targets)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # Calculate R2 score\n",
    "            val_targets_np = val_targets.cpu().numpy()\n",
    "            val_outputs_np = val_outputs.cpu().numpy()\n",
    "\n",
    "\n",
    "            val_all_predictions.extend(val_outputs_np)\n",
    "            val_all_targets.extend(val_targets_np)\n",
    "            num_sequences+=1\n",
    "\n",
    "        average_val_loss = total_val_loss / num_sequences\n",
    "        r_squared = r2_score(val_all_targets, val_all_predictions)\n",
    "\n",
    "        print(\"--------------Validation----------\")\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {average_val_loss:.4f}, R2 Score: {r_squared:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd-CLMDpZSaK"
   },
   "source": [
    "In PyTorch, the unsqueeze(0) method is used to add a new dimension of size 1 at the specified position. In this case, it adds a new dimension at the beginning of the tensor targets. The resulting tensor has a size of (1, original_size).\n",
    "\n",
    "The repeat(50, 1, 1) method is then applied to this tensor. This function creates a new tensor by repeating the original tensor along each specified dimension. In this case, it repeats the tensor 50 times along the first dimension, and once along the second and third dimensions.\n",
    "\n",
    "So, if targets initially had a size of (original_size), after unsqueeze(0).repeat(50, 1, 1), the resulting tensor will have a size of (50, original_size). The values in the tensor will be the same across the repeated dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N57UkwuQYbLP",
    "outputId": "962e6f0b-9305-412d-b44a-1b8ecb183c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 2.1018, R-squared: 0.1687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_test_loss=0\n",
    "    num_sequences=0\n",
    "    for test_inputs, test_targets in test_text_dataloader:\n",
    "        # test_targets=test_targets.unsqueeze(1).repeat(1, 50, 1)\n",
    "        test_outputs = model(test_inputs)\n",
    "        test_loss=Loss(test_outputs,test_targets)\n",
    "        total_test_loss+=test_loss\n",
    "        # Convert predictions and targets to numpy arrays\n",
    "        predictions = test_outputs.numpy().flatten()\n",
    "        targets = test_targets.numpy().flatten()\n",
    "\n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(targets)\n",
    "        num_sequences+=1\n",
    "\n",
    "    mae=total_test_loss/num_sequences\n",
    "    r_squared = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    print(f'Test MAE: {mae:.4f}, R-squared: {r_squared:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syvnSkdAdzzA"
   },
   "source": [
    "Test MAE: 1.0795, R-squared: 0.2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2WUXu0OYyqJ"
   },
   "source": [
    "# Saving the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8Rlev7zY02x"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/content/drive/MyDrive/multi_model_SA/final_model_MAE-1.0795_R-0.2011.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
