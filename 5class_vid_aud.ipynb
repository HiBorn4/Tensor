{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvh3WK8R-uQm",
    "outputId": "2d7c22fa-4e6d-4d07-d6c3-976755699969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MultiBench'...\n",
      "remote: Enumerating objects: 6943, done.\u001b[K\n",
      "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 6943 (delta 72), reused 121 (delta 60), pack-reused 6789\u001b[K\n",
      "Receiving objects: 100% (6943/6943), 51.07 MiB | 16.73 MiB/s, done.\n",
      "Resolving deltas: 100% (4258/4258), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pliang279/MultiBench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glSa5-my-6Lu",
    "outputId": "4c2d0980-6c5f-4001-89d0-8242bcea44fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/MultiBench\n"
     ]
    }
   ],
   "source": [
    "%cd MultiBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoAWYmLo-7oT",
    "outputId": "f30a8a09-9351-40f9-8e19-c5065c7ed084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.3)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU\n",
      "From (redirected): https://drive.google.com/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU&confirm=t&uuid=ba87ac79-8e60-4e3c-930e-2d7a816e2720\n",
      "To: /content/MultiBench/mosi_raw.pkl\n",
      "100% 357M/357M [00:01<00:00, 182MB/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!pip install gdown && gdown https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRm1lyMN-9po"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import TensorDataset,DataLoader,ConcatDataset\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdrs3p9H-_n3"
   },
   "outputs": [],
   "source": [
    "from datasets.affect.get_data import get_dataloader\n",
    "traindata, validdata, testdata = get_dataloader(\n",
    "    '/content/MultiBench/mosi_raw.pkl', robust_test=False, max_pad=True, data_type='mosi', max_seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFlJa9mU_F8u"
   },
   "outputs": [],
   "source": [
    "for batch in traindata:\n",
    "  audio,video,text,labels=batch\n",
    "\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29nFIG3EL7ma"
   },
   "source": [
    "**Audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0EUC-JHBJhE"
   },
   "outputs": [],
   "source": [
    "all_audio = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in traindata:\n",
    "    audio, labels = batch[0], batch[-1]  # Extract first and last component\n",
    "    all_audio.append(audio)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_audio = torch.cat(all_audio, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "rounded_labels = torch.round(all_labels)\n",
    "\n",
    "# Step 2: Clamp the values to the desired range [-2, 2]\n",
    "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize an empty tensor to store the one-hot encoded labels\n",
    "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
    "\n",
    "# Iterate over each clamped label and set the corresponding index to 1\n",
    "for i, label in enumerate(clamped_labels):\n",
    "    # Extract the integer value from the tensor\n",
    "    int_label = int(label.item())\n",
    "\n",
    "    # Shift the label range from [-2, 2] to [0, 4]\n",
    "    shifted_label = int_label + 2\n",
    "    one_hot_labels[i][shifted_label] = 1\n",
    "new_dataset = TensorDataset(all_audio, one_hot_labels)\n",
    "train_audio_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIM8L-um_729",
    "outputId": "f43972ac-e5b1-458b-8460-a1d0eba06b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.],\n",
      "        [ 2.],\n",
      "        [ 0.],\n",
      "        ...,\n",
      "        [ 2.],\n",
      "        [-1.],\n",
      "        [-1.]])\n"
     ]
    }
   ],
   "source": [
    "print(clamped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieAQ_DShBZaw",
    "outputId": "5328acd5-6f48-4015-c102-2bedd94b03c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(train_audio_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVyvov0JBdfq"
   },
   "outputs": [],
   "source": [
    "all_audio = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in validdata:\n",
    "    audio, labels = batch[0], batch[-1]  # Extract first and last component\n",
    "    all_audio.append(audio)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_audio = torch.cat(all_audio, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "rounded_labels = torch.round(all_labels)\n",
    "\n",
    "# Step 2: Clamp the values to the desired range [-2, 2]\n",
    "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize an empty tensor to store the one-hot encoded labels\n",
    "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
    "\n",
    "# Iterate over each clamped label and set the corresponding index to 1\n",
    "for i, label in enumerate(clamped_labels):\n",
    "    # Extract the integer value from the tensor\n",
    "    int_label = int(label.item())\n",
    "\n",
    "    # Shift the label range from [-2, 2] to [0, 4]\n",
    "    shifted_label = int_label + 2\n",
    "    one_hot_labels[i][shifted_label] = 1\n",
    "new_dataset = TensorDataset(all_audio, one_hot_labels)\n",
    "val_audio_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kS5ZyLBRBzNT",
    "outputId": "698c1804-273d-4b17-d32c-e34865425dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(val_audio_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk1qhuQ6B4XP"
   },
   "outputs": [],
   "source": [
    "all_audio = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in testdata:\n",
    "    audio, labels = batch[0], batch[-1]  # Extract first and last component\n",
    "    all_audio.append(audio)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_audio = torch.cat(all_audio, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "rounded_labels = torch.round(all_labels)\n",
    "\n",
    "# Step 2: Clamp the values to the desired range [-2, 2]\n",
    "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize an empty tensor to store the one-hot encoded labels\n",
    "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
    "\n",
    "# Iterate over each clamped label and set the corresponding index to 1\n",
    "for i, label in enumerate(clamped_labels):\n",
    "    # Extract the integer value from the tensor\n",
    "    int_label = int(label.item())\n",
    "\n",
    "    # Shift the label range from [-2, 2] to [0, 4]\n",
    "    shifted_label = int_label + 2\n",
    "    one_hot_labels[i][shifted_label] = 1\n",
    "new_dataset = TensorDataset(all_audio, one_hot_labels)\n",
    "test_audio_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYUjJjTZCCil",
    "outputId": "1b874937-defe-47ec-a3a3-a8ab31393244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(test_audio_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBt0b-dhCII4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SubnetModel(nn.Module):\n",
    "    def __init__(self, input_size, num_utterances, fc1_size, fc2_size, fc3_size):\n",
    "        super(SubnetModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, fc3_size)\n",
    "\n",
    "        # Final output layer with 5 neurons and sigmoid activation\n",
    "        self.output_layer = nn.Linear(fc3_size, 5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x, dim=1)  # Take the mean along the utterance dimension\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        fc1_out = self.relu(self.fc1(x))\n",
    "        fc2_out = self.relu(self.fc2(fc1_out))\n",
    "        fc3_out = self.relu(self.fc3(fc2_out))\n",
    "\n",
    "        # Final output layer with sigmoid activation\n",
    "        output = self.sigmoid(self.output_layer(fc3_out))\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycMmCfXCC77_",
    "outputId": "6af31994-efb1-4314-f634-b73215e9ec94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubnetModel(\n",
      "  (fc1): Linear(in_features=35, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (output_layer): Linear(in_features=32, out_features=5, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 20\n",
    "audio_params = (35, max_seq_len, 32, 32, 32)\n",
    "\n",
    "# Initialize the model\n",
    "model = SubnetModel(*audio_params)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwrdoA8YK36l"
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters())[2:], lr=0.00006)\n",
    "num_epochs = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1GcHyIsLAWt",
    "outputId": "17a27956-ae6c-47d5-ca7a-33a1609406c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Training Loss: 1.6077, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [2/150], Training Loss: 1.6075, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [3/150], Training Loss: 1.6070, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [4/150], Training Loss: 1.6066, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [5/150], Training Loss: 1.6066, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [6/150], Training Loss: 1.6066, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [7/150], Training Loss: 1.6063, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [8/150], Training Loss: 1.6060, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [9/150], Training Loss: 1.6059, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [10/150], Training Loss: 1.6050, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [11/150], Training Loss: 1.6051, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [12/150], Training Loss: 1.6046, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [13/150], Training Loss: 1.6048, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [14/150], Training Loss: 1.6042, Training Accuracy: 0.0008, Validation Accuracy: 0.0000\n",
      "Epoch [15/150], Training Loss: 1.6037, Training Accuracy: 0.0016, Validation Accuracy: 0.0000\n",
      "Epoch [16/150], Training Loss: 1.6035, Training Accuracy: 0.0016, Validation Accuracy: 0.0000\n",
      "Epoch [17/150], Training Loss: 1.6025, Training Accuracy: 0.0086, Validation Accuracy: 0.0000\n",
      "Epoch [18/150], Training Loss: 1.6028, Training Accuracy: 0.0234, Validation Accuracy: 0.0140\n",
      "Epoch [19/150], Training Loss: 1.6023, Training Accuracy: 0.0335, Validation Accuracy: 0.0234\n",
      "Epoch [20/150], Training Loss: 1.6020, Training Accuracy: 0.0460, Validation Accuracy: 0.0280\n",
      "Epoch [21/150], Training Loss: 1.6019, Training Accuracy: 0.0624, Validation Accuracy: 0.0327\n",
      "Epoch [22/150], Training Loss: 1.6011, Training Accuracy: 0.0889, Validation Accuracy: 0.0514\n",
      "Epoch [23/150], Training Loss: 1.5992, Training Accuracy: 0.1154, Validation Accuracy: 0.0841\n",
      "Epoch [24/150], Training Loss: 1.6017, Training Accuracy: 0.1372, Validation Accuracy: 0.1215\n",
      "Epoch [25/150], Training Loss: 1.5995, Training Accuracy: 0.1652, Validation Accuracy: 0.1542\n",
      "Epoch [26/150], Training Loss: 1.6004, Training Accuracy: 0.1847, Validation Accuracy: 0.1963\n",
      "Epoch [27/150], Training Loss: 1.6001, Training Accuracy: 0.1964, Validation Accuracy: 0.2243\n",
      "Epoch [28/150], Training Loss: 1.5999, Training Accuracy: 0.2065, Validation Accuracy: 0.2196\n",
      "Epoch [29/150], Training Loss: 1.5983, Training Accuracy: 0.2112, Validation Accuracy: 0.2336\n",
      "Epoch [30/150], Training Loss: 1.5973, Training Accuracy: 0.2245, Validation Accuracy: 0.2336\n",
      "Epoch [31/150], Training Loss: 1.5981, Training Accuracy: 0.2253, Validation Accuracy: 0.2383\n",
      "Epoch [32/150], Training Loss: 1.5985, Training Accuracy: 0.2323, Validation Accuracy: 0.2383\n",
      "Epoch [33/150], Training Loss: 1.5970, Training Accuracy: 0.2432, Validation Accuracy: 0.2617\n",
      "Epoch [34/150], Training Loss: 1.5964, Training Accuracy: 0.2471, Validation Accuracy: 0.2664\n",
      "Epoch [35/150], Training Loss: 1.5960, Training Accuracy: 0.2479, Validation Accuracy: 0.2710\n",
      "Epoch [36/150], Training Loss: 1.5958, Training Accuracy: 0.2494, Validation Accuracy: 0.2757\n",
      "Epoch [37/150], Training Loss: 1.5967, Training Accuracy: 0.2479, Validation Accuracy: 0.2757\n",
      "Epoch [38/150], Training Loss: 1.5967, Training Accuracy: 0.2486, Validation Accuracy: 0.2757\n",
      "Epoch [39/150], Training Loss: 1.5937, Training Accuracy: 0.2518, Validation Accuracy: 0.2757\n",
      "Epoch [40/150], Training Loss: 1.5956, Training Accuracy: 0.2557, Validation Accuracy: 0.2804\n",
      "Epoch [41/150], Training Loss: 1.5951, Training Accuracy: 0.2588, Validation Accuracy: 0.2850\n",
      "Epoch [42/150], Training Loss: 1.5933, Training Accuracy: 0.2595, Validation Accuracy: 0.2944\n",
      "Epoch [43/150], Training Loss: 1.5914, Training Accuracy: 0.2611, Validation Accuracy: 0.2897\n",
      "Epoch [44/150], Training Loss: 1.5914, Training Accuracy: 0.2627, Validation Accuracy: 0.2991\n",
      "Epoch [45/150], Training Loss: 1.5936, Training Accuracy: 0.2666, Validation Accuracy: 0.2991\n",
      "Epoch [46/150], Training Loss: 1.5916, Training Accuracy: 0.2634, Validation Accuracy: 0.2991\n",
      "Epoch [47/150], Training Loss: 1.5933, Training Accuracy: 0.2650, Validation Accuracy: 0.2991\n",
      "Epoch [48/150], Training Loss: 1.5899, Training Accuracy: 0.2673, Validation Accuracy: 0.2991\n",
      "Epoch [49/150], Training Loss: 1.5914, Training Accuracy: 0.2673, Validation Accuracy: 0.2991\n",
      "Epoch [50/150], Training Loss: 1.5869, Training Accuracy: 0.2658, Validation Accuracy: 0.2991\n",
      "Epoch [51/150], Training Loss: 1.5899, Training Accuracy: 0.2642, Validation Accuracy: 0.2991\n",
      "Epoch [52/150], Training Loss: 1.5905, Training Accuracy: 0.2666, Validation Accuracy: 0.2991\n",
      "Epoch [53/150], Training Loss: 1.5893, Training Accuracy: 0.2650, Validation Accuracy: 0.3037\n",
      "Epoch [54/150], Training Loss: 1.5892, Training Accuracy: 0.2650, Validation Accuracy: 0.3084\n",
      "Epoch [55/150], Training Loss: 1.5885, Training Accuracy: 0.2650, Validation Accuracy: 0.3084\n",
      "Epoch [56/150], Training Loss: 1.5881, Training Accuracy: 0.2658, Validation Accuracy: 0.3084\n",
      "Epoch [57/150], Training Loss: 1.5894, Training Accuracy: 0.2658, Validation Accuracy: 0.3084\n",
      "Epoch [58/150], Training Loss: 1.5872, Training Accuracy: 0.2650, Validation Accuracy: 0.3084\n",
      "Epoch [59/150], Training Loss: 1.5865, Training Accuracy: 0.2634, Validation Accuracy: 0.3037\n",
      "Epoch [60/150], Training Loss: 1.5860, Training Accuracy: 0.2642, Validation Accuracy: 0.3037\n",
      "Epoch [61/150], Training Loss: 1.5852, Training Accuracy: 0.2627, Validation Accuracy: 0.3037\n",
      "Epoch [62/150], Training Loss: 1.5881, Training Accuracy: 0.2611, Validation Accuracy: 0.2991\n",
      "Epoch [63/150], Training Loss: 1.5853, Training Accuracy: 0.2595, Validation Accuracy: 0.2991\n",
      "Epoch [64/150], Training Loss: 1.5865, Training Accuracy: 0.2603, Validation Accuracy: 0.2991\n",
      "Epoch [65/150], Training Loss: 1.5865, Training Accuracy: 0.2603, Validation Accuracy: 0.2991\n",
      "Epoch [66/150], Training Loss: 1.5869, Training Accuracy: 0.2564, Validation Accuracy: 0.2897\n",
      "Epoch [67/150], Training Loss: 1.5846, Training Accuracy: 0.2588, Validation Accuracy: 0.2897\n",
      "Epoch [68/150], Training Loss: 1.5795, Training Accuracy: 0.2557, Validation Accuracy: 0.2897\n",
      "Epoch [69/150], Training Loss: 1.5846, Training Accuracy: 0.2588, Validation Accuracy: 0.2897\n",
      "Epoch [70/150], Training Loss: 1.5845, Training Accuracy: 0.2533, Validation Accuracy: 0.2897\n",
      "Epoch [71/150], Training Loss: 1.5841, Training Accuracy: 0.2510, Validation Accuracy: 0.2897\n",
      "Epoch [72/150], Training Loss: 1.5833, Training Accuracy: 0.2510, Validation Accuracy: 0.2897\n",
      "Epoch [73/150], Training Loss: 1.5841, Training Accuracy: 0.2494, Validation Accuracy: 0.2897\n",
      "Epoch [74/150], Training Loss: 1.5820, Training Accuracy: 0.2486, Validation Accuracy: 0.2804\n",
      "Epoch [75/150], Training Loss: 1.5816, Training Accuracy: 0.2494, Validation Accuracy: 0.2804\n",
      "Epoch [76/150], Training Loss: 1.5798, Training Accuracy: 0.2447, Validation Accuracy: 0.2757\n",
      "Epoch [77/150], Training Loss: 1.5802, Training Accuracy: 0.2471, Validation Accuracy: 0.2804\n",
      "Epoch [78/150], Training Loss: 1.5781, Training Accuracy: 0.2463, Validation Accuracy: 0.2804\n",
      "Epoch [79/150], Training Loss: 1.5774, Training Accuracy: 0.2447, Validation Accuracy: 0.2804\n",
      "Epoch [80/150], Training Loss: 1.5824, Training Accuracy: 0.2463, Validation Accuracy: 0.2804\n",
      "Epoch [81/150], Training Loss: 1.5805, Training Accuracy: 0.2440, Validation Accuracy: 0.2757\n",
      "Epoch [82/150], Training Loss: 1.5822, Training Accuracy: 0.2463, Validation Accuracy: 0.2757\n",
      "Epoch [83/150], Training Loss: 1.5817, Training Accuracy: 0.2463, Validation Accuracy: 0.2804\n",
      "Epoch [84/150], Training Loss: 1.5790, Training Accuracy: 0.2455, Validation Accuracy: 0.2804\n",
      "Epoch [85/150], Training Loss: 1.5779, Training Accuracy: 0.2424, Validation Accuracy: 0.2850\n",
      "Epoch [86/150], Training Loss: 1.5803, Training Accuracy: 0.2424, Validation Accuracy: 0.2804\n",
      "Epoch [87/150], Training Loss: 1.5768, Training Accuracy: 0.2385, Validation Accuracy: 0.2757\n",
      "Epoch [88/150], Training Loss: 1.5777, Training Accuracy: 0.2385, Validation Accuracy: 0.2757\n",
      "Epoch [89/150], Training Loss: 1.5806, Training Accuracy: 0.2377, Validation Accuracy: 0.2757\n",
      "Epoch [90/150], Training Loss: 1.5795, Training Accuracy: 0.2369, Validation Accuracy: 0.2757\n",
      "Epoch [91/150], Training Loss: 1.5746, Training Accuracy: 0.2362, Validation Accuracy: 0.2757\n",
      "Epoch [92/150], Training Loss: 1.5739, Training Accuracy: 0.2362, Validation Accuracy: 0.2804\n",
      "Epoch [93/150], Training Loss: 1.5756, Training Accuracy: 0.2362, Validation Accuracy: 0.2664\n",
      "Epoch [94/150], Training Loss: 1.5747, Training Accuracy: 0.2362, Validation Accuracy: 0.2710\n",
      "Epoch [95/150], Training Loss: 1.5763, Training Accuracy: 0.2385, Validation Accuracy: 0.2710\n",
      "Epoch [96/150], Training Loss: 1.5774, Training Accuracy: 0.2346, Validation Accuracy: 0.2710\n",
      "Epoch [97/150], Training Loss: 1.5755, Training Accuracy: 0.2346, Validation Accuracy: 0.2710\n",
      "Epoch [98/150], Training Loss: 1.5725, Training Accuracy: 0.2354, Validation Accuracy: 0.2710\n",
      "Epoch [99/150], Training Loss: 1.5766, Training Accuracy: 0.2369, Validation Accuracy: 0.2710\n",
      "Epoch [100/150], Training Loss: 1.5776, Training Accuracy: 0.2369, Validation Accuracy: 0.2710\n",
      "Epoch [101/150], Training Loss: 1.5771, Training Accuracy: 0.2362, Validation Accuracy: 0.2710\n",
      "Epoch [102/150], Training Loss: 1.5771, Training Accuracy: 0.2338, Validation Accuracy: 0.2664\n",
      "Epoch [103/150], Training Loss: 1.5662, Training Accuracy: 0.2338, Validation Accuracy: 0.2664\n",
      "Epoch [104/150], Training Loss: 1.5727, Training Accuracy: 0.2362, Validation Accuracy: 0.2664\n",
      "Epoch [105/150], Training Loss: 1.5699, Training Accuracy: 0.2362, Validation Accuracy: 0.2664\n",
      "Epoch [106/150], Training Loss: 1.5737, Training Accuracy: 0.2315, Validation Accuracy: 0.2664\n",
      "Epoch [107/150], Training Loss: 1.5758, Training Accuracy: 0.2307, Validation Accuracy: 0.2664\n",
      "Epoch [108/150], Training Loss: 1.5717, Training Accuracy: 0.2307, Validation Accuracy: 0.2664\n",
      "Epoch [109/150], Training Loss: 1.5753, Training Accuracy: 0.2299, Validation Accuracy: 0.2664\n",
      "Epoch [110/150], Training Loss: 1.5701, Training Accuracy: 0.2292, Validation Accuracy: 0.2664\n",
      "Epoch [111/150], Training Loss: 1.5715, Training Accuracy: 0.2292, Validation Accuracy: 0.2664\n",
      "Epoch [112/150], Training Loss: 1.5701, Training Accuracy: 0.2292, Validation Accuracy: 0.2617\n",
      "Epoch [113/150], Training Loss: 1.5709, Training Accuracy: 0.2292, Validation Accuracy: 0.2617\n",
      "Epoch [114/150], Training Loss: 1.5706, Training Accuracy: 0.2229, Validation Accuracy: 0.2617\n",
      "Epoch [115/150], Training Loss: 1.5713, Training Accuracy: 0.2229, Validation Accuracy: 0.2617\n",
      "Epoch [116/150], Training Loss: 1.5673, Training Accuracy: 0.2229, Validation Accuracy: 0.2617\n",
      "Epoch [117/150], Training Loss: 1.5720, Training Accuracy: 0.2237, Validation Accuracy: 0.2617\n",
      "Epoch [118/150], Training Loss: 1.5721, Training Accuracy: 0.2253, Validation Accuracy: 0.2664\n",
      "Epoch [119/150], Training Loss: 1.5693, Training Accuracy: 0.2221, Validation Accuracy: 0.2617\n",
      "Epoch [120/150], Training Loss: 1.5727, Training Accuracy: 0.2214, Validation Accuracy: 0.2617\n",
      "Epoch [121/150], Training Loss: 1.5703, Training Accuracy: 0.2221, Validation Accuracy: 0.2664\n",
      "Epoch [122/150], Training Loss: 1.5679, Training Accuracy: 0.2182, Validation Accuracy: 0.2617\n",
      "Epoch [123/150], Training Loss: 1.5671, Training Accuracy: 0.2167, Validation Accuracy: 0.2617\n",
      "Epoch [124/150], Training Loss: 1.5694, Training Accuracy: 0.2175, Validation Accuracy: 0.2570\n",
      "Epoch [125/150], Training Loss: 1.5697, Training Accuracy: 0.2182, Validation Accuracy: 0.2617\n",
      "Epoch [126/150], Training Loss: 1.5682, Training Accuracy: 0.2167, Validation Accuracy: 0.2570\n",
      "Epoch [127/150], Training Loss: 1.5669, Training Accuracy: 0.2136, Validation Accuracy: 0.2477\n",
      "Epoch [128/150], Training Loss: 1.5702, Training Accuracy: 0.2159, Validation Accuracy: 0.2523\n",
      "Epoch [129/150], Training Loss: 1.5709, Training Accuracy: 0.2143, Validation Accuracy: 0.2523\n",
      "Epoch [130/150], Training Loss: 1.5691, Training Accuracy: 0.2143, Validation Accuracy: 0.2477\n",
      "Epoch [131/150], Training Loss: 1.5672, Training Accuracy: 0.2143, Validation Accuracy: 0.2477\n",
      "Epoch [132/150], Training Loss: 1.5645, Training Accuracy: 0.2104, Validation Accuracy: 0.2430\n",
      "Epoch [133/150], Training Loss: 1.5678, Training Accuracy: 0.2097, Validation Accuracy: 0.2383\n",
      "Epoch [134/150], Training Loss: 1.5690, Training Accuracy: 0.2081, Validation Accuracy: 0.2383\n",
      "Epoch [135/150], Training Loss: 1.5648, Training Accuracy: 0.2073, Validation Accuracy: 0.2383\n",
      "Epoch [136/150], Training Loss: 1.5606, Training Accuracy: 0.2065, Validation Accuracy: 0.2383\n",
      "Epoch [137/150], Training Loss: 1.5642, Training Accuracy: 0.2081, Validation Accuracy: 0.2383\n",
      "Epoch [138/150], Training Loss: 1.5640, Training Accuracy: 0.2081, Validation Accuracy: 0.2383\n",
      "Epoch [139/150], Training Loss: 1.5603, Training Accuracy: 0.2073, Validation Accuracy: 0.2383\n",
      "Epoch [140/150], Training Loss: 1.5643, Training Accuracy: 0.2081, Validation Accuracy: 0.2383\n",
      "Epoch [141/150], Training Loss: 1.5659, Training Accuracy: 0.2081, Validation Accuracy: 0.2383\n",
      "Epoch [142/150], Training Loss: 1.5681, Training Accuracy: 0.2089, Validation Accuracy: 0.2383\n",
      "Epoch [143/150], Training Loss: 1.5670, Training Accuracy: 0.2089, Validation Accuracy: 0.2383\n",
      "Epoch [144/150], Training Loss: 1.5629, Training Accuracy: 0.2089, Validation Accuracy: 0.2383\n",
      "Epoch [145/150], Training Loss: 1.5682, Training Accuracy: 0.2081, Validation Accuracy: 0.2383\n",
      "Epoch [146/150], Training Loss: 1.5677, Training Accuracy: 0.2081, Validation Accuracy: 0.2430\n",
      "Epoch [147/150], Training Loss: 1.5654, Training Accuracy: 0.2104, Validation Accuracy: 0.2430\n",
      "Epoch [148/150], Training Loss: 1.5677, Training Accuracy: 0.2089, Validation Accuracy: 0.2430\n",
      "Epoch [149/150], Training Loss: 1.5603, Training Accuracy: 0.2097, Validation Accuracy: 0.2430\n",
      "Epoch [150/150], Training Loss: 1.5625, Training Accuracy: 0.2136, Validation Accuracy: 0.2523\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize variables for epoch loss and accuracy calculation\n",
    "    total_loss = 0.0\n",
    "    correct_predictions_train = 0\n",
    "    total_predictions_train = 0\n",
    "\n",
    "    # Iterate over the training dataset\n",
    "    for inputs, labels in train_audio_dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert outputs to one-hot format\n",
    "        predicted_labels_train = (outputs >= 0.5).float()\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct_predictions_train += (predicted_labels_train == labels).all(dim=1).sum().item()\n",
    "\n",
    "        # Update total samples\n",
    "        total_predictions_train += labels.size(0)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = total_loss / len(train_audio_dataloader)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_predictions_train / total_predictions_train\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct_predictions_val = 0\n",
    "        total_predictions_val = 0\n",
    "\n",
    "        # Iterate over the validation dataset\n",
    "        for inputs, labels in val_audio_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels_val = (outputs >= 0.5).float()\n",
    "            correct_predictions_val += (predicted_labels_val == labels).all(dim=1).sum().item()\n",
    "            total_predictions_val += labels.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_predictions_val / total_predictions_val\n",
    "\n",
    "    # Print epoch information\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giZQ4dQALFfK",
    "outputId": "ad0ebd01-98e4-4fb8-fed6-5ae2de90a80b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1224\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct_predictions_test = 0\n",
    "    total_predictions_test = 0\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    for inputs, labels in test_audio_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted_labels_test = (outputs >= 0.5).float()\n",
    "        correct_predictions_test += (predicted_labels_test == labels).all(dim=1).sum().item()\n",
    "        total_predictions_test += labels.size(0)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = correct_predictions_test / total_predictions_test\n",
    "\n",
    "# Print test accuracy\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlLEBOV3L_UV"
   },
   "source": [
    "**Video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1Vh5Y8oLchV"
   },
   "outputs": [],
   "source": [
    "all_video = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in traindata:\n",
    "    video, labels = batch[1], batch[-1]  # Extract first and last component\n",
    "    all_video.append(video)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_video = torch.cat(all_video, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "rounded_labels = torch.round(all_labels)\n",
    "\n",
    "# Step 2: Clamp the values to the desired range [-2, 2]\n",
    "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize an empty tensor to store the one-hot encoded labels\n",
    "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
    "\n",
    "# Iterate over each clamped label and set the corresponding index to 1\n",
    "for i, label in enumerate(clamped_labels):\n",
    "    # Extract the integer value from the tensor\n",
    "    int_label = int(label.item())\n",
    "\n",
    "    # Shift the label range from [-2, 2] to [0, 4]\n",
    "    shifted_label = int_label + 2\n",
    "    one_hot_labels[i][shifted_label] = 1\n",
    "new_dataset = TensorDataset(all_video, one_hot_labels)\n",
    "train_video_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48nZmV3iMXjm",
    "outputId": "da4d3670-5d4d-431d-8441-d254f05869ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(train_video_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwEjT58xMcky"
   },
   "outputs": [],
   "source": [
    "all_video = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in validdata:\n",
    "    video, labels = batch[1], batch[-1]  # Extract first and last component\n",
    "    all_video.append(video)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_video = torch.cat(all_video, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "rounded_labels = torch.round(all_labels)\n",
    "\n",
    "# Step 2: Clamp the values to the desired range [-2, 2]\n",
    "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize an empty tensor to store the one-hot encoded labels\n",
    "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
    "\n",
    "# Iterate over each clamped label and set the corresponding index to 1\n",
    "for i, label in enumerate(clamped_labels):\n",
    "    # Extract the integer value from the tensor\n",
    "    int_label = int(label.item())\n",
    "\n",
    "    # Shift the label range from [-2, 2] to [0, 4]\n",
    "    shifted_label = int_label + 2\n",
    "    one_hot_labels[i][shifted_label] = 1\n",
    "new_dataset = TensorDataset(all_video, one_hot_labels)\n",
    "valid_video_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNleyDOlMjoL",
    "outputId": "3aea8a0f-e626-424a-cdab-59823811f8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_video_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxM-gkuxMnVE"
   },
   "outputs": [],
   "source": [
    "all_video = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in testdata:\n",
    "    video, labels = batch[1], batch[-1]  # Extract first and last component\n",
    "    all_video.append(video)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_video = torch.cat(all_video, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "rounded_labels = torch.round(all_labels)\n",
    "\n",
    "# Step 2: Clamp the values to the desired range [-2, 2]\n",
    "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize an empty tensor to store the one-hot encoded labels\n",
    "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
    "\n",
    "# Iterate over each clamped label and set the corresponding index to 1\n",
    "for i, label in enumerate(clamped_labels):\n",
    "    # Extract the integer value from the tensor\n",
    "    int_label = int(label.item())\n",
    "\n",
    "    # Shift the label range from [-2, 2] to [0, 4]\n",
    "    shifted_label = int_label + 2\n",
    "    one_hot_labels[i][shifted_label] = 1\n",
    "new_dataset = TensorDataset(all_video, one_hot_labels)\n",
    "test_video_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLEQhDb6My1c",
    "outputId": "6713ca38-e4b2-47ef-fb97-b7375af8c6b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(test_video_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvQ7XkzsM7Tn",
    "outputId": "d831df00-16ad-4315-88d0-6d0d9273521f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubnetModel(\n",
      "  (fc1): Linear(in_features=35, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (output_layer): Linear(in_features=32, out_features=5, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 20\n",
    "video_params=(35,max_seq_len,32,32,32)\n",
    "\n",
    "# Initialize the model\n",
    "model = SubnetModel(*video_params)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0x9hsweoNVOO"
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters())[2:], lr=0.00006)\n",
    "num_epochs = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnneozfTNbjA",
    "outputId": "60f18586-5d93-4d31-c46f-8bca38f50819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Training Loss: 1.6100, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [2/150], Training Loss: 1.6089, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [3/150], Training Loss: 1.6097, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [4/150], Training Loss: 1.6095, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [5/150], Training Loss: 1.6087, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [6/150], Training Loss: 1.6085, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [7/150], Training Loss: 1.6083, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [8/150], Training Loss: 1.6079, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [9/150], Training Loss: 1.6075, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [10/150], Training Loss: 1.6072, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [11/150], Training Loss: 1.6071, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [12/150], Training Loss: 1.6072, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [13/150], Training Loss: 1.6070, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [14/150], Training Loss: 1.6069, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [15/150], Training Loss: 1.6063, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [16/150], Training Loss: 1.6060, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [17/150], Training Loss: 1.6064, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [18/150], Training Loss: 1.6048, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [19/150], Training Loss: 1.6060, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [20/150], Training Loss: 1.6047, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [21/150], Training Loss: 1.6039, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [22/150], Training Loss: 1.6047, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [23/150], Training Loss: 1.6040, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [24/150], Training Loss: 1.6031, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [25/150], Training Loss: 1.6042, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [26/150], Training Loss: 1.6032, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [27/150], Training Loss: 1.6028, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [28/150], Training Loss: 1.6024, Training Accuracy: 0.0000, Validation Accuracy: 0.0000\n",
      "Epoch [29/150], Training Loss: 1.6010, Training Accuracy: 0.0000, Validation Accuracy: 0.0047\n",
      "Epoch [30/150], Training Loss: 1.6019, Training Accuracy: 0.0000, Validation Accuracy: 0.0047\n",
      "Epoch [31/150], Training Loss: 1.6012, Training Accuracy: 0.0000, Validation Accuracy: 0.0047\n",
      "Epoch [32/150], Training Loss: 1.6000, Training Accuracy: 0.0000, Validation Accuracy: 0.0047\n",
      "Epoch [33/150], Training Loss: 1.6005, Training Accuracy: 0.0000, Validation Accuracy: 0.0047\n",
      "Epoch [34/150], Training Loss: 1.6002, Training Accuracy: 0.0000, Validation Accuracy: 0.0047\n",
      "Epoch [35/150], Training Loss: 1.5997, Training Accuracy: 0.0000, Validation Accuracy: 0.0047\n",
      "Epoch [36/150], Training Loss: 1.5991, Training Accuracy: 0.0008, Validation Accuracy: 0.0047\n",
      "Epoch [37/150], Training Loss: 1.5989, Training Accuracy: 0.0016, Validation Accuracy: 0.0093\n",
      "Epoch [38/150], Training Loss: 1.5984, Training Accuracy: 0.0039, Validation Accuracy: 0.0140\n",
      "Epoch [39/150], Training Loss: 1.5981, Training Accuracy: 0.0070, Validation Accuracy: 0.0234\n",
      "Epoch [40/150], Training Loss: 1.5975, Training Accuracy: 0.0117, Validation Accuracy: 0.0234\n",
      "Epoch [41/150], Training Loss: 1.5975, Training Accuracy: 0.0164, Validation Accuracy: 0.0234\n",
      "Epoch [42/150], Training Loss: 1.5957, Training Accuracy: 0.0265, Validation Accuracy: 0.0374\n",
      "Epoch [43/150], Training Loss: 1.5960, Training Accuracy: 0.0374, Validation Accuracy: 0.0421\n",
      "Epoch [44/150], Training Loss: 1.5958, Training Accuracy: 0.0421, Validation Accuracy: 0.0467\n",
      "Epoch [45/150], Training Loss: 1.5946, Training Accuracy: 0.0616, Validation Accuracy: 0.0561\n",
      "Epoch [46/150], Training Loss: 1.5951, Training Accuracy: 0.0795, Validation Accuracy: 0.0561\n",
      "Epoch [47/150], Training Loss: 1.5955, Training Accuracy: 0.1138, Validation Accuracy: 0.0935\n",
      "Epoch [48/150], Training Loss: 1.5956, Training Accuracy: 0.1481, Validation Accuracy: 0.1308\n",
      "Epoch [49/150], Training Loss: 1.5950, Training Accuracy: 0.1761, Validation Accuracy: 0.1776\n",
      "Epoch [50/150], Training Loss: 1.5921, Training Accuracy: 0.1980, Validation Accuracy: 0.2150\n",
      "Epoch [51/150], Training Loss: 1.5951, Training Accuracy: 0.2159, Validation Accuracy: 0.2477\n",
      "Epoch [52/150], Training Loss: 1.5945, Training Accuracy: 0.2245, Validation Accuracy: 0.2477\n",
      "Epoch [53/150], Training Loss: 1.5936, Training Accuracy: 0.2323, Validation Accuracy: 0.2477\n",
      "Epoch [54/150], Training Loss: 1.5902, Training Accuracy: 0.2393, Validation Accuracy: 0.2523\n",
      "Epoch [55/150], Training Loss: 1.5915, Training Accuracy: 0.2447, Validation Accuracy: 0.2617\n",
      "Epoch [56/150], Training Loss: 1.5881, Training Accuracy: 0.2471, Validation Accuracy: 0.2570\n",
      "Epoch [57/150], Training Loss: 1.5921, Training Accuracy: 0.2518, Validation Accuracy: 0.2664\n",
      "Epoch [58/150], Training Loss: 1.5921, Training Accuracy: 0.2533, Validation Accuracy: 0.2664\n",
      "Epoch [59/150], Training Loss: 1.5912, Training Accuracy: 0.2525, Validation Accuracy: 0.2710\n",
      "Epoch [60/150], Training Loss: 1.5888, Training Accuracy: 0.2557, Validation Accuracy: 0.2757\n",
      "Epoch [61/150], Training Loss: 1.5875, Training Accuracy: 0.2588, Validation Accuracy: 0.2757\n",
      "Epoch [62/150], Training Loss: 1.5880, Training Accuracy: 0.2595, Validation Accuracy: 0.2757\n",
      "Epoch [63/150], Training Loss: 1.5899, Training Accuracy: 0.2611, Validation Accuracy: 0.2757\n",
      "Epoch [64/150], Training Loss: 1.5879, Training Accuracy: 0.2611, Validation Accuracy: 0.2757\n",
      "Epoch [65/150], Training Loss: 1.5878, Training Accuracy: 0.2634, Validation Accuracy: 0.2804\n",
      "Epoch [66/150], Training Loss: 1.5883, Training Accuracy: 0.2634, Validation Accuracy: 0.2757\n",
      "Epoch [67/150], Training Loss: 1.5879, Training Accuracy: 0.2650, Validation Accuracy: 0.2897\n",
      "Epoch [68/150], Training Loss: 1.5854, Training Accuracy: 0.2634, Validation Accuracy: 0.2757\n",
      "Epoch [69/150], Training Loss: 1.5862, Training Accuracy: 0.2634, Validation Accuracy: 0.2757\n",
      "Epoch [70/150], Training Loss: 1.5850, Training Accuracy: 0.2627, Validation Accuracy: 0.2757\n",
      "Epoch [71/150], Training Loss: 1.5847, Training Accuracy: 0.2634, Validation Accuracy: 0.2757\n",
      "Epoch [72/150], Training Loss: 1.5871, Training Accuracy: 0.2642, Validation Accuracy: 0.2804\n",
      "Epoch [73/150], Training Loss: 1.5860, Training Accuracy: 0.2642, Validation Accuracy: 0.2757\n",
      "Epoch [74/150], Training Loss: 1.5811, Training Accuracy: 0.2642, Validation Accuracy: 0.2757\n",
      "Epoch [75/150], Training Loss: 1.5830, Training Accuracy: 0.2642, Validation Accuracy: 0.2804\n",
      "Epoch [76/150], Training Loss: 1.5809, Training Accuracy: 0.2642, Validation Accuracy: 0.2804\n",
      "Epoch [77/150], Training Loss: 1.5848, Training Accuracy: 0.2650, Validation Accuracy: 0.2850\n",
      "Epoch [78/150], Training Loss: 1.5857, Training Accuracy: 0.2650, Validation Accuracy: 0.2804\n",
      "Epoch [79/150], Training Loss: 1.5823, Training Accuracy: 0.2650, Validation Accuracy: 0.2897\n",
      "Epoch [80/150], Training Loss: 1.5824, Training Accuracy: 0.2658, Validation Accuracy: 0.2897\n",
      "Epoch [81/150], Training Loss: 1.5830, Training Accuracy: 0.2666, Validation Accuracy: 0.2944\n",
      "Epoch [82/150], Training Loss: 1.5817, Training Accuracy: 0.2658, Validation Accuracy: 0.2710\n",
      "Epoch [83/150], Training Loss: 1.5816, Training Accuracy: 0.2658, Validation Accuracy: 0.2850\n",
      "Epoch [84/150], Training Loss: 1.5807, Training Accuracy: 0.2666, Validation Accuracy: 0.2850\n",
      "Epoch [85/150], Training Loss: 1.5816, Training Accuracy: 0.2666, Validation Accuracy: 0.2664\n",
      "Epoch [86/150], Training Loss: 1.5796, Training Accuracy: 0.2666, Validation Accuracy: 0.2757\n",
      "Epoch [87/150], Training Loss: 1.5815, Training Accuracy: 0.2666, Validation Accuracy: 0.2757\n",
      "Epoch [88/150], Training Loss: 1.5808, Training Accuracy: 0.2666, Validation Accuracy: 0.2757\n",
      "Epoch [89/150], Training Loss: 1.5785, Training Accuracy: 0.2658, Validation Accuracy: 0.2664\n",
      "Epoch [90/150], Training Loss: 1.5819, Training Accuracy: 0.2627, Validation Accuracy: 0.2710\n",
      "Epoch [91/150], Training Loss: 1.5785, Training Accuracy: 0.2627, Validation Accuracy: 0.2710\n",
      "Epoch [92/150], Training Loss: 1.5758, Training Accuracy: 0.2627, Validation Accuracy: 0.2710\n",
      "Epoch [93/150], Training Loss: 1.5769, Training Accuracy: 0.2634, Validation Accuracy: 0.2710\n",
      "Epoch [94/150], Training Loss: 1.5806, Training Accuracy: 0.2627, Validation Accuracy: 0.2710\n",
      "Epoch [95/150], Training Loss: 1.5773, Training Accuracy: 0.2627, Validation Accuracy: 0.2710\n",
      "Epoch [96/150], Training Loss: 1.5765, Training Accuracy: 0.2627, Validation Accuracy: 0.2710\n",
      "Epoch [97/150], Training Loss: 1.5766, Training Accuracy: 0.2619, Validation Accuracy: 0.2710\n",
      "Epoch [98/150], Training Loss: 1.5738, Training Accuracy: 0.2619, Validation Accuracy: 0.2710\n",
      "Epoch [99/150], Training Loss: 1.5782, Training Accuracy: 0.2619, Validation Accuracy: 0.2710\n",
      "Epoch [100/150], Training Loss: 1.5729, Training Accuracy: 0.2619, Validation Accuracy: 0.2664\n",
      "Epoch [101/150], Training Loss: 1.5784, Training Accuracy: 0.2611, Validation Accuracy: 0.2710\n",
      "Epoch [102/150], Training Loss: 1.5758, Training Accuracy: 0.2603, Validation Accuracy: 0.2617\n",
      "Epoch [103/150], Training Loss: 1.5785, Training Accuracy: 0.2580, Validation Accuracy: 0.2570\n",
      "Epoch [104/150], Training Loss: 1.5741, Training Accuracy: 0.2580, Validation Accuracy: 0.2570\n",
      "Epoch [105/150], Training Loss: 1.5744, Training Accuracy: 0.2588, Validation Accuracy: 0.2617\n",
      "Epoch [106/150], Training Loss: 1.5785, Training Accuracy: 0.2603, Validation Accuracy: 0.2617\n",
      "Epoch [107/150], Training Loss: 1.5738, Training Accuracy: 0.2595, Validation Accuracy: 0.2617\n",
      "Epoch [108/150], Training Loss: 1.5746, Training Accuracy: 0.2588, Validation Accuracy: 0.2617\n",
      "Epoch [109/150], Training Loss: 1.5683, Training Accuracy: 0.2580, Validation Accuracy: 0.2570\n",
      "Epoch [110/150], Training Loss: 1.5676, Training Accuracy: 0.2588, Validation Accuracy: 0.2570\n",
      "Epoch [111/150], Training Loss: 1.5758, Training Accuracy: 0.2603, Validation Accuracy: 0.2617\n",
      "Epoch [112/150], Training Loss: 1.5751, Training Accuracy: 0.2580, Validation Accuracy: 0.2570\n",
      "Epoch [113/150], Training Loss: 1.5754, Training Accuracy: 0.2580, Validation Accuracy: 0.2570\n",
      "Epoch [114/150], Training Loss: 1.5708, Training Accuracy: 0.2572, Validation Accuracy: 0.2570\n",
      "Epoch [115/150], Training Loss: 1.5689, Training Accuracy: 0.2572, Validation Accuracy: 0.2570\n",
      "Epoch [116/150], Training Loss: 1.5711, Training Accuracy: 0.2572, Validation Accuracy: 0.2570\n",
      "Epoch [117/150], Training Loss: 1.5732, Training Accuracy: 0.2572, Validation Accuracy: 0.2523\n",
      "Epoch [118/150], Training Loss: 1.5703, Training Accuracy: 0.2557, Validation Accuracy: 0.2523\n",
      "Epoch [119/150], Training Loss: 1.5740, Training Accuracy: 0.2557, Validation Accuracy: 0.2523\n",
      "Epoch [120/150], Training Loss: 1.5715, Training Accuracy: 0.2557, Validation Accuracy: 0.2523\n",
      "Epoch [121/150], Training Loss: 1.5717, Training Accuracy: 0.2557, Validation Accuracy: 0.2523\n",
      "Epoch [122/150], Training Loss: 1.5741, Training Accuracy: 0.2557, Validation Accuracy: 0.2523\n",
      "Epoch [123/150], Training Loss: 1.5729, Training Accuracy: 0.2557, Validation Accuracy: 0.2523\n",
      "Epoch [124/150], Training Loss: 1.5723, Training Accuracy: 0.2549, Validation Accuracy: 0.2523\n",
      "Epoch [125/150], Training Loss: 1.5726, Training Accuracy: 0.2549, Validation Accuracy: 0.2523\n",
      "Epoch [126/150], Training Loss: 1.5696, Training Accuracy: 0.2549, Validation Accuracy: 0.2523\n",
      "Epoch [127/150], Training Loss: 1.5687, Training Accuracy: 0.2541, Validation Accuracy: 0.2477\n",
      "Epoch [128/150], Training Loss: 1.5697, Training Accuracy: 0.2557, Validation Accuracy: 0.2523\n",
      "Epoch [129/150], Training Loss: 1.5704, Training Accuracy: 0.2549, Validation Accuracy: 0.2523\n",
      "Epoch [130/150], Training Loss: 1.5667, Training Accuracy: 0.2549, Validation Accuracy: 0.2477\n",
      "Epoch [131/150], Training Loss: 1.5702, Training Accuracy: 0.2541, Validation Accuracy: 0.2477\n",
      "Epoch [132/150], Training Loss: 1.5709, Training Accuracy: 0.2549, Validation Accuracy: 0.2477\n",
      "Epoch [133/150], Training Loss: 1.5648, Training Accuracy: 0.2549, Validation Accuracy: 0.2477\n",
      "Epoch [134/150], Training Loss: 1.5701, Training Accuracy: 0.2549, Validation Accuracy: 0.2477\n",
      "Epoch [135/150], Training Loss: 1.5703, Training Accuracy: 0.2541, Validation Accuracy: 0.2477\n",
      "Epoch [136/150], Training Loss: 1.5714, Training Accuracy: 0.2533, Validation Accuracy: 0.2477\n",
      "Epoch [137/150], Training Loss: 1.5680, Training Accuracy: 0.2525, Validation Accuracy: 0.2477\n",
      "Epoch [138/150], Training Loss: 1.5697, Training Accuracy: 0.2525, Validation Accuracy: 0.2477\n",
      "Epoch [139/150], Training Loss: 1.5679, Training Accuracy: 0.2525, Validation Accuracy: 0.2477\n",
      "Epoch [140/150], Training Loss: 1.5690, Training Accuracy: 0.2510, Validation Accuracy: 0.2477\n",
      "Epoch [141/150], Training Loss: 1.5670, Training Accuracy: 0.2510, Validation Accuracy: 0.2477\n",
      "Epoch [142/150], Training Loss: 1.5679, Training Accuracy: 0.2518, Validation Accuracy: 0.2477\n",
      "Epoch [143/150], Training Loss: 1.5627, Training Accuracy: 0.2510, Validation Accuracy: 0.2477\n",
      "Epoch [144/150], Training Loss: 1.5655, Training Accuracy: 0.2510, Validation Accuracy: 0.2477\n",
      "Epoch [145/150], Training Loss: 1.5705, Training Accuracy: 0.2518, Validation Accuracy: 0.2477\n",
      "Epoch [146/150], Training Loss: 1.5645, Training Accuracy: 0.2510, Validation Accuracy: 0.2477\n",
      "Epoch [147/150], Training Loss: 1.5673, Training Accuracy: 0.2502, Validation Accuracy: 0.2477\n",
      "Epoch [148/150], Training Loss: 1.5642, Training Accuracy: 0.2518, Validation Accuracy: 0.2477\n",
      "Epoch [149/150], Training Loss: 1.5705, Training Accuracy: 0.2533, Validation Accuracy: 0.2477\n",
      "Epoch [150/150], Training Loss: 1.5638, Training Accuracy: 0.2533, Validation Accuracy: 0.2477\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize variables for epoch loss and accuracy calculation\n",
    "    total_loss = 0.0\n",
    "    correct_predictions_train = 0\n",
    "    total_predictions_train = 0\n",
    "\n",
    "    # Iterate over the training dataset\n",
    "    for inputs, labels in train_audio_dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert outputs to one-hot format\n",
    "        predicted_labels_train = (outputs >= 0.5).float()\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct_predictions_train += (predicted_labels_train == labels).all(dim=1).sum().item()\n",
    "\n",
    "        # Update total samples\n",
    "        total_predictions_train += labels.size(0)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = total_loss / len(train_audio_dataloader)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_predictions_train / total_predictions_train\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct_predictions_val = 0\n",
    "        total_predictions_val = 0\n",
    "\n",
    "        # Iterate over the validation dataset\n",
    "        for inputs, labels in val_audio_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels_val = (outputs >= 0.5).float()\n",
    "            correct_predictions_val += (predicted_labels_val == labels).all(dim=1).sum().item()\n",
    "            total_predictions_val += labels.size(0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = correct_predictions_val / total_predictions_val\n",
    "\n",
    "    # Print epoch information\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxLgia3gNhUx",
    "outputId": "ea552936-ad28-4d4a-e841-4e73d316ffb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1312\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct_predictions_test = 0\n",
    "    total_predictions_test = 0\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    for inputs, labels in test_audio_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted_labels_test = (outputs >= 0.5).float()\n",
    "        correct_predictions_test += (predicted_labels_test == labels).all(dim=1).sum().item()\n",
    "        total_predictions_test += labels.size(0)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = correct_predictions_test / total_predictions_test\n",
    "\n",
    "# Print test accuracy\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsjKVqqdOAsY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
