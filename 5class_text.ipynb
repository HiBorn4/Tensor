{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7x9FBKvYzlF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKkMw8-_xPu-",
        "outputId": "2810f81e-1713-425a-b3d7-d71980ec0b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
            "  File \"/tmp/ipykernel_5889/2091075093.py\", line 1, in <module>\n",
            "    get_ipython().system('git clone https://github.com/pliang279/MultiBench.git')\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 657, in system_piped\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/utils/_process_posix.py\", line 125, in system\n",
            "ModuleNotFoundError: No module named 'pexpect'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
            "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1454, in structured_traceback\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1345, in structured_traceback\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1192, in structured_traceback\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1133, in get_records\n",
            "  File \"/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/myenv/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
            "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pliang279/MultiBench.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzwmUuMVxhoA",
        "outputId": "64f46000-07e8-4e3f-e50d-1b6993ae2d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/hi-born4/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/MultiBench\n"
          ]
        }
      ],
      "source": [
        "%cd MultiBench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O7LPLWXxjY1",
        "outputId": "d79065e8-1508-4c9c-c89a-02b3556feb45"
      },
      "outputs": [],
      "source": [
        "# %mkdir data\n",
        "# %pip install gdown && gdown https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hDnc5K1IxnVw"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZeQIMr9mxqkd"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "dynamic module does not define module export function (PyInit__torchtext)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maffect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mget_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataloader\n\u001b[1;32m      2\u001b[0m traindata, validdata, testdata \u001b[38;5;241m=\u001b[39m get_dataloader(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmosi_raw.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, robust_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmosi\u001b[39m\u001b[38;5;124m'\u001b[39m, max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "File \u001b[0;32m~/6th Sem/Applied Linear Algebra for Machine Learning/Tensor fusion network for multi modal sentiment analysis/MultiBench/datasets/affect/get_data.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/torchtext/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      8\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/torchtext/_extension.py:61\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
            "\u001b[0;31mImportError\u001b[0m: dynamic module does not define module export function (PyInit__torchtext)"
          ]
        }
      ],
      "source": [
        "from datasets.affect.get_data import get_dataloader\n",
        "traindata, validdata, testdata = get_dataloader(\n",
        "    'mosi_raw.pkl', robust_test=False, max_pad=True, data_type='mosi', max_seq_len=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JGg4FYcxyeL",
        "outputId": "4181b0fb-bfed-4057-8e4d-ef27d9f4022b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n",
            "22\n"
          ]
        }
      ],
      "source": [
        "print(len(traindata)+len(validdata)+len(testdata))\n",
        "print(len(testdata))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckApJl_wx4gZ"
      },
      "outputs": [],
      "source": [
        "for batch in traindata:\n",
        "  audio,video,text,labels=batch\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDm7nSoWqp1d"
      },
      "outputs": [],
      "source": [
        "all_text = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in traindata:\n",
        "    text, labels = batch[-2:]\n",
        "    all_text.append(text)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "all_text = torch.cat(all_text, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53JrIu1brR3-",
        "outputId": "677bc300-288b-4301-da27-6f738f81fb4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1283\n"
          ]
        }
      ],
      "source": [
        "print(len(all_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYan-9-Lqxp9",
        "outputId": "8ebdffe8-f462-425a-ab74-f88c4944adb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        ...,\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Step 1: Round the floating-point values to the nearest integer\n",
        "rounded_labels = torch.round(all_labels)\n",
        "\n",
        "# Step 2: Clamp the values to the desired range [-2, 2]\n",
        "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
        "\n",
        "print(clamped_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AldTawDsKRU",
        "outputId": "1c0341eb-b7f9-4125-b348-d513789e6f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1283, 1])\n"
          ]
        }
      ],
      "source": [
        "print(clamped_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8OO0-_7r_4p",
        "outputId": "54da431d-7d7c-4761-c3f7-637e78028ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        ...,\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "num_classes = 5\n",
        "\n",
        "# Initialize an empty tensor to store the one-hot encoded labels\n",
        "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
        "\n",
        "# Iterate over each clamped label and set the corresponding index to 1\n",
        "for i, label in enumerate(clamped_labels):\n",
        "    # Extract the integer value from the tensor\n",
        "    int_label = int(label.item())\n",
        "\n",
        "    # Shift the label range from [-2, 2] to [0, 4]\n",
        "    shifted_label = int_label + 2\n",
        "    one_hot_labels[i][shifted_label] = 1\n",
        "\n",
        "print(one_hot_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2lGQS5vZ1OD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset,DataLoader,ConcatDataset\n",
        "new_dataset = TensorDataset(all_text, one_hot_labels)\n",
        "train_text_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_ejsC2SaBS8",
        "outputId": "ecd372e5-cd90-4b4a-f330-a29fb5e78909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41\n"
          ]
        }
      ],
      "source": [
        "print(len(train_text_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZHOOYfRaUVF",
        "outputId": "af61c54b-1c69-4cd5-e7ac-954c773846ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "214\n",
            "tensor([[ 2.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [ 2.]])\n",
            "tensor([[0., 0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "all_text = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in validdata:\n",
        "    text, labels = batch[-2:]  # Text and labels are the last two components\n",
        "    all_text.append(text)\n",
        "    all_labels.append(labels)  # Round the labels\n",
        "\n",
        "# Concatenate all batches\n",
        "all_text = torch.cat(all_text, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "print(len(all_text))\n",
        "rounded_labels = torch.round(all_labels)\n",
        "\n",
        "# Step 2: Clamp the values to the desired range [-2, 2]\n",
        "clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
        "\n",
        "print(clamped_labels)\n",
        "num_classes = 5\n",
        "\n",
        "# Initialize an empty tensor to store the one-hot encoded labels\n",
        "one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
        "\n",
        "# Iterate over each clamped label and set the corresponding index to 1\n",
        "for i, label in enumerate(clamped_labels):\n",
        "    # Extract the integer value from the tensor\n",
        "    int_label = int(label.item())\n",
        "\n",
        "    # Shift the label range from [-2, 2] to [0, 4]\n",
        "    shifted_label = int_label + 2\n",
        "    one_hot_labels[i][shifted_label] = 1\n",
        "\n",
        "print(one_hot_labels)\n",
        "from torch.utils.data import TensorDataset,DataLoader,ConcatDataset\n",
        "new_dataset = TensorDataset(all_text, one_hot_labels)\n",
        "val_text_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YYV-OHsa78Y",
        "outputId": "332a3161-fd94-44af-b736-0e481c685897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "print(len(val_text_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrr6qgTYbINl",
        "outputId": "a529ca29-59b1-445e-f0f1-a9345f229e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "686\n",
            "tensor([[-2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [-0.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-0.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-2.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 2.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [-1.],\n",
            "        [ 1.],\n",
            "        [-0.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-2.],\n",
            "        [-2.],\n",
            "        [ 1.],\n",
            "        [ 1.],\n",
            "        [-1.],\n",
            "        [-1.],\n",
            "        [ 1.]])\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.]])\n"
          ]
        }
      ],
      "source": [
        "  all_text = []\n",
        "  all_labels = []\n",
        "\n",
        "  for batch in testdata:\n",
        "      text, labels = batch[-2:]  # Text and labels are the last two components\n",
        "      all_text.append(text)\n",
        "      all_labels.append(labels)  # Round the labels\n",
        "\n",
        "  # Concatenate all batches\n",
        "  all_text = torch.cat(all_text, dim=0)\n",
        "  all_labels = torch.cat(all_labels, dim=0)\n",
        "  print(len(all_text))\n",
        "  rounded_labels = torch.round(all_labels)\n",
        "\n",
        "  # Step 2: Clamp the values to the desired range [-2, 2]\n",
        "  clamped_labels = torch.clamp(rounded_labels, min=-2, max=2)\n",
        "\n",
        "  print(clamped_labels)\n",
        "  num_classes = 5\n",
        "\n",
        "  # Initialize an empty tensor to store the one-hot encoded labels\n",
        "  one_hot_labels = torch.zeros((len(clamped_labels), num_classes))\n",
        "\n",
        "  # Iterate over each clamped label and set the corresponding index to 1\n",
        "  for i, label in enumerate(clamped_labels):\n",
        "      # Extract the integer value from the tensor\n",
        "      int_label = int(label.item())\n",
        "\n",
        "      # Shift the label range from [-2, 2] to [0, 4]\n",
        "      shifted_label = int_label + 2\n",
        "      one_hot_labels[i][shifted_label] = 1\n",
        "\n",
        "  print(one_hot_labels)\n",
        "  from torch.utils.data import TensorDataset,DataLoader,ConcatDataset\n",
        "  new_dataset = TensorDataset(all_text, one_hot_labels)\n",
        "  test_text_dataloader = DataLoader(new_dataset, batch_size=32,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qZ-mqrRbZ2-",
        "outputId": "9ab61295-c320-4447-9488-bddb544bd158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "print(len(test_text_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8uDZF5lbsZ2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, fc1_size, fc2_size, si_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        # LSTM layer (stacked LSTM)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, fc1_size)\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
        "        self.si1 = nn.Linear(fc2_size, si_size)\n",
        "        self.si2 = nn.Linear(si_size, si_size)\n",
        "\n",
        "        # Output layer with 5 neurons\n",
        "        self.output_layer = nn.Linear(si_size, 5)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        fc1_out = self.relu(self.fc1(lstm_out[:, -1, :]))\n",
        "        fc2_out = self.relu(self.fc2(fc1_out))\n",
        "\n",
        "        si1 = self.relu(self.si1(fc2_out))\n",
        "        si2 = self.relu(self.si2(si1))\n",
        "\n",
        "        # Output layer with sigmoid activation\n",
        "        output = self.sigmoid(self.output_layer(si2))\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp2ugXzocWKp",
        "outputId": "9d3b4b25-5532-415d-87e5-d116d885f10c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LanguageModel(\n",
            "  (lstm): LSTM(300, 128, batch_first=True)\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (si1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (si2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# model parameters\n",
        "\n",
        "#LSTM parameters\n",
        "input_size = 300 # since each word is in GloVe embedding format\n",
        "hidden_size = 128 # hyperparameter for each LSTM layer. refers to the dimension of the hidden state value\n",
        "num_layers=1\n",
        "\n",
        "fc1_size = 128\n",
        "fc2_size = 128\n",
        "si_size=128\n",
        "\n",
        "model = LanguageModel(input_size, hidden_size,num_layers, fc1_size, fc2_size,si_size)\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbmrWgNvchqm"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters())[2:], lr=0.00006)\n",
        "num_epochs = 150\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkNMxsFQhS0b",
        "outputId": "52733e29-ab80-469f-801c-f5cdfba4f8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Training Loss: 1.5931, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [2/50], Training Loss: 1.5834, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [3/50], Training Loss: 1.5890, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [4/50], Training Loss: 1.5891, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [5/50], Training Loss: 1.5931, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [6/50], Training Loss: 1.5899, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [7/50], Training Loss: 1.5918, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [8/50], Training Loss: 1.5924, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [9/50], Training Loss: 1.5923, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [10/50], Training Loss: 1.5923, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [11/50], Training Loss: 1.5889, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [12/50], Training Loss: 1.5861, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [13/50], Training Loss: 1.5932, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [14/50], Training Loss: 1.5914, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [15/50], Training Loss: 1.5923, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [16/50], Training Loss: 1.5914, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [17/50], Training Loss: 1.5919, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [18/50], Training Loss: 1.5926, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [19/50], Training Loss: 1.5912, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [20/50], Training Loss: 1.5922, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [21/50], Training Loss: 1.5887, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [22/50], Training Loss: 1.5895, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [23/50], Training Loss: 1.5892, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [24/50], Training Loss: 1.5884, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [25/50], Training Loss: 1.5929, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [26/50], Training Loss: 1.5895, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [27/50], Training Loss: 1.5919, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [28/50], Training Loss: 1.5918, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [29/50], Training Loss: 1.5919, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [30/50], Training Loss: 1.5893, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [31/50], Training Loss: 1.5919, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [32/50], Training Loss: 1.5893, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [33/50], Training Loss: 1.5924, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [34/50], Training Loss: 1.5924, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [35/50], Training Loss: 1.5914, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [36/50], Training Loss: 1.5860, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [37/50], Training Loss: 1.5888, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [38/50], Training Loss: 1.5853, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [39/50], Training Loss: 1.5919, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [40/50], Training Loss: 1.5911, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [41/50], Training Loss: 1.5916, Training Accuracy: 0.2814, Validation Accuracy: 0.3178\n",
            "Epoch [42/50], Training Loss: 1.5857, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [43/50], Training Loss: 1.5923, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [44/50], Training Loss: 1.5886, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [45/50], Training Loss: 1.5854, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [46/50], Training Loss: 1.5847, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [47/50], Training Loss: 1.5882, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [48/50], Training Loss: 1.5853, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [49/50], Training Loss: 1.5880, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n",
            "Epoch [50/50], Training Loss: 1.5924, Training Accuracy: 0.2822, Validation Accuracy: 0.3178\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize variables for epoch loss and accuracy calculation\n",
        "    total_loss = 0.0\n",
        "    correct_predictions_train = 0\n",
        "    total_predictions_train = 0\n",
        "\n",
        "    # Iterate over the training dataset\n",
        "    for inputs, labels in train_text_dataloader:\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Convert outputs to one-hot format\n",
        "        predicted_labels_train = (outputs >= 0.5).float()\n",
        "\n",
        "        # Count correct predictions\n",
        "        correct_predictions_train += (predicted_labels_train == labels).all(dim=1).sum().item()\n",
        "\n",
        "        # Update total samples\n",
        "        total_predictions_train += labels.size(0)\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    average_loss = total_loss / len(train_text_dataloader)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = correct_predictions_train / total_predictions_train\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        correct_predictions_val = 0\n",
        "        total_predictions_val = 0\n",
        "\n",
        "        # Iterate over the validation dataset\n",
        "        for inputs, labels in val_text_dataloader:\n",
        "            outputs = model(inputs)\n",
        "            predicted_labels_val = (outputs >= 0.5).float()\n",
        "            correct_predictions_val += (predicted_labels_val == labels).all(dim=1).sum().item()\n",
        "            total_predictions_val += labels.size(0)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        val_accuracy = correct_predictions_val / total_predictions_val\n",
        "\n",
        "    # Print epoch information\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9kDAk3ChYsg",
        "outputId": "f69d4140-ac6b-42e8-f56c-1e4b40220a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.1778\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on test dataset\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    correct_predictions_test = 0\n",
        "    total_predictions_test = 0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    for inputs, labels in test_text_dataloader:\n",
        "        outputs = model(inputs)\n",
        "        predicted_labels_test = (outputs >= 0.5).float()\n",
        "        correct_predictions_test += (predicted_labels_test == labels).all(dim=1).sum().item()\n",
        "        total_predictions_test += labels.size(0)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    test_accuracy = correct_predictions_test / total_predictions_test\n",
        "\n",
        "# Print test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY7KsplZhewf"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/My Drive/languagemodel.pt'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAJIqtNsicz7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
