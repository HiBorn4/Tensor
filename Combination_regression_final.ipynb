{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYs7BSKW5Gqk"
   },
   "source": [
    "\n",
    "**Loading** CMU-MOSI **dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f80j9mDfld4U",
    "outputId": "acbcfd92-2af1-4499-c0e3-6377ca734494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MultiBench'...\n",
      "remote: Enumerating objects: 6943, done.\u001b[K\n",
      "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 6943 (delta 72), reused 121 (delta 60), pack-reused 6789\u001b[K\n",
      "Receiving objects: 100% (6943/6943), 51.07 MiB | 15.42 MiB/s, done.\n",
      "Resolving deltas: 100% (4258/4258), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pliang279/MultiBench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4h7C4XAqNTq",
    "outputId": "1a0a2b8a-2891-4a0f-c5bf-ffb2c2ae6d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/MultiBench\n"
     ]
    }
   ],
   "source": [
    "%cd MultiBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbL-Wu23sEAO",
    "outputId": "8f75f155-8677-417d-9282-fbf25095f3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "# !mkdir data\n",
    "# !pip install gdown && gdown https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT1jN1eqsJio"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye24eAGqsP3p"
   },
   "outputs": [],
   "source": [
    "# Import the associated dataloader for affect datasets, which MOSI is a part of.\n",
    "from datasets.affect.get_data import get_dataloader\n",
    "\n",
    "# Create the training, validation, and test-set dataloaders.\n",
    "traindata, validdata, testdata = get_dataloader(\n",
    "    '/content/drive/MyDrive/multi_model_SA/mosi_raw.pkl', robust_test=False, max_pad=True, data_type='mosi', max_seq_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFkqO1W-hI9n"
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEkie_f1cqcG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SubnetModel(nn.Module):\n",
    "    def __init__(self,input_size,num_utterances,fc1_size, fc2_size,fc3_size):\n",
    "        super(SubnetModel, self).__init__()\n",
    "\n",
    "        # self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "        # self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "        # # self.weight_decay=weight_decay\n",
    "\n",
    "        # self.norm = nn.BatchNorm1d(input_size)\n",
    "        self.drop = nn.Dropout(p=0.15)\n",
    "\n",
    "        # Fully connected layers\n",
    "\n",
    "        #fc1 gets hidden_size dimension values as input\n",
    "        self.fc1 = nn.Linear(input_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, fc3_size)\n",
    "\n",
    "        # Output layer\n",
    "        # self.output_layer = nn.Linear(fc3_size, 1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x=x.view(x.size(0),-1) # flattening the input to feed to the fully connected layer as it expects 1D input\n",
    "        # print(x.shape)\n",
    "        # normed = self.norm(x)\n",
    "        # dropped = self.drop(normed)\n",
    "\n",
    "        # x = F.normalize(x, p=2, dim=1)\n",
    "        # print(\"x shape\")\n",
    "        # print(x.shape)\n",
    "        x = torch.mean(x, dim=1)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        # nan_mask = torch.isnan(x)\n",
    "        # # Replace NaN values with zeros\n",
    "        # x[nan_mask] = 0\n",
    "\n",
    "        # normed = self.norm(x)\n",
    "\n",
    "        #3 hidden layers\n",
    "\n",
    "        fc1_out = self.relu(self.fc1(x))\n",
    "        drop1=self.drop(fc1_out)\n",
    "\n",
    "        fc2_out = self.relu(self.fc2(drop1))\n",
    "        drop2=self.drop(fc2_out)\n",
    "\n",
    "        fc3_out=self.relu(self.fc3(drop2))\n",
    "        drop3=self.drop(fc3_out)\n",
    "\n",
    "        return drop3\n",
    "\n",
    "        # # Output layer with Sigmoid activation\n",
    "        # output = self.sigmoid(self.output_layer(fc3_out))\n",
    "\n",
    "        # # get output between -3 and +3\n",
    "\n",
    "        # output=output*self.output_range+self.output_shift\n",
    "\n",
    "\n",
    "        # return output\n",
    "\n",
    "    # def l2_regularization_loss(self):\n",
    "    #     l2_loss = 0.0\n",
    "    #     for param in self.parameters():\n",
    "    #         l2_loss += torch.norm(param, p=2)  # L2 norm of the parameters\n",
    "    #     return self.weight_decay * l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcVZJRGndvbw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers,fc1_size, fc2_size):\n",
    "        super(TextModel, self).__init__()\n",
    "\n",
    "        # self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "        # self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "        # LSTM layer (stacked LSTM)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.drop = nn.Dropout(p=0.15)\n",
    "\n",
    "        #fc1 gets hidden_size dimension values as input\n",
    "        self.fc1 = nn.Linear(hidden_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "\n",
    "        # Output layer\n",
    "        # self.output_layer = nn.Linear(fc2_size, 1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1, x.size(-1), x.size(-2))\n",
    "\n",
    "        # LSTM layer\n",
    "\n",
    "        lstm_out, (hidden_states, cell_states) = self.lstm(x)\n",
    "        # last_time_step_index = lstm_out.size(1) - 1\n",
    "\n",
    "        # # # Extract the output of the final LSTM unit\n",
    "        # final_lstm_output = lstm_out[:, last_time_step_index, :]\n",
    "\n",
    "        # print(lstm_out.shape)\n",
    "        # print(hidden_states.shape)\n",
    "        # print(type(hidden_states))\n",
    "\n",
    "        #print(hidden_states.squeeze().shape)\n",
    "\n",
    "        fc1_out = self.relu(self.fc1(hidden_states.squeeze()))\n",
    "        # drop1=self.drop(fc1_out)\n",
    "\n",
    "        fc2_out = self.relu(self.fc2(fc1_out))\n",
    "        # drop2=self.drop(fc2_out)\n",
    "\n",
    "\n",
    "        return fc2_out\n",
    "\n",
    "        # # Output layer with Sigmoid activation\n",
    "        # output = self.sigmoid(self.output_layer(fc2_out))\n",
    "        # #get output between -3 and +3\n",
    "        # output=output*self.output_range+self.output_shift\n",
    "\n",
    "        # return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0LDfUqSga0P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class TFN(nn.Module):\n",
    "    def __init__(self,audio_params,video_params,text_params,SIN_params):\n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "        self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "        self.audio_params=audio_params\n",
    "        self.video_params=video_params\n",
    "        self.text_params=text_params\n",
    "\n",
    "        #unimodels\n",
    "        self.audio_subnet=SubnetModel(audio_params[0],audio_params[1],audio_params[2],audio_params[3],audio_params[4])\n",
    "        self.video_subnet=SubnetModel(video_params[0],video_params[1],video_params[2],video_params[3],video_params[4])\n",
    "        self.text_subnet=TextModel(text_params[0],text_params[1],text_params[2],text_params[3],text_params[4])\n",
    "\n",
    "        # Fully connected layers\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.15)\n",
    "\n",
    "        #fc1 gets hidden_size dimension values as input\n",
    "        self.fc1 = nn.Linear(((audio_params[2]+1)*(video_params[2]+1)*(text_params[3]+1)), SIN_params[0])\n",
    "        self.fc2 = nn.Linear(SIN_params[0], SIN_params[1])\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(SIN_params[1], 1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        DTYPE = torch.FloatTensor\n",
    "\n",
    "        batch_size=x[0].shape[0]\n",
    "\n",
    "        # unimodal outputs\n",
    "\n",
    "        audio_out=self.audio_subnet(x[0])\n",
    "        video_out=self.video_subnet(x[1])\n",
    "        text_out=self.text_subnet(x[2])\n",
    "\n",
    "        # adding 1 to increase the dimension value\n",
    "\n",
    "        audio_out = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), audio_out), dim=1)\n",
    "        # print(\"audio_out\")\n",
    "        # print(audio_out.shape)\n",
    "        video_out = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), video_out), dim=1)\n",
    "        # print(\"video_out\")\n",
    "        # print(video_out.shape)\n",
    "        text_out = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), text_out), dim=1)\n",
    "        # print(\"text_out\")\n",
    "        # print(text_out.shape)\n",
    "\n",
    "\n",
    "        # tensorfusion operation\n",
    "\n",
    "        fusion_tensor = torch.bmm(audio_out.unsqueeze(2), video_out.unsqueeze(1))\n",
    "        # print(\"fusion tensor 1\")\n",
    "        # print(fusion_tensor.shape)\n",
    "        fusion_tensor = fusion_tensor.view(-1, (self.audio_params[2] + 1) * (self.video_params[2] + 1), 1)\n",
    "        # print(\"fusion tensor 2\")\n",
    "        # print(fusion_tensor.shape)\n",
    "        fusion_tensor = torch.bmm(fusion_tensor, text_out.unsqueeze(1)).view(batch_size, -1)\n",
    "        # print(\"fusion tensor 3\")\n",
    "        # print(fusion_tensor.shape)\n",
    "\n",
    "        # print(\"fusion tensor:\")\n",
    "        # print(fusion_tensor)\n",
    "        # sentiment inference network (SIN)\n",
    "\n",
    "        fc1_out = self.relu(self.fc1(fusion_tensor))\n",
    "        drop1=self.drop(fc1_out)\n",
    "\n",
    "        fc2_out = self.relu(self.fc2(drop1))\n",
    "        drop2=self.drop(fc2_out)\n",
    "\n",
    "        # Output layer with Sigmoid activation\n",
    "        output = self.sigmoid(self.output_layer(drop2))\n",
    "\n",
    "        # print(\"output\")\n",
    "        # print(\"output.shape\")\n",
    "\n",
    "        # get output between -3 and +3\n",
    "        output=(output*self.output_range)+self.output_shift\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GM5QzJGBCaYt"
   },
   "source": [
    "audio_out\n",
    "\n",
    "torch.Size([32, 33])\n",
    "\n",
    "then changed to torch.Size([32,33,1])\n",
    "\n",
    "video_out\n",
    "\n",
    "torch.Size([32, 33])\n",
    "\n",
    "then changed to torch.Size([32,1,33])\n",
    "\n",
    "text_out\n",
    "\n",
    "torch.Size([32, 129])\n",
    "\n",
    "fusion tensor 1\n",
    "\n",
    "torch.Size([32, 33, 33])\n",
    "\n",
    "fusion tensor 2\n",
    "\n",
    "torch.Size([32, 1089, 1])\n",
    "\n",
    "fusion tensor 3\n",
    "\n",
    "first changed text_out to torch.Size([32,1,129]) then bmm with fusion tensor 2\n",
    "\n",
    "we get torch.Size([32,1089,129]) which is then flattened to:\n",
    "\n",
    "torch.Size([32, 140481]) to feed to fully connected network\n",
    "\n",
    "which is torch.Size([32,129x33x33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf3Zk87ghD2O"
   },
   "source": [
    "**-------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv6JFvPMo9Sr"
   },
   "outputs": [],
   "source": [
    "max_seq_len=20\n",
    "\n",
    "audio_params=(35,max_seq_len,32,32,32) # (feature_length,max_seq_len,fc1_size,fc2_size,fc3_size)\n",
    "video_params=(74,max_seq_len,32,32,32) # (feature_length,max_seq_len,fc1_size,fc2_size,fc3_size)\n",
    "text_params=(300,128,1,128,128) # (feature_length,LSTM_hidden_size,num_LSTM_layers,fc1_size,fc2_size)\n",
    "\n",
    "SIN_params=(128,128)\n",
    "\n",
    "final_model=TFN(audio_params,video_params,text_params,SIN_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YObAFSTrCwM"
   },
   "source": [
    "loading weights of saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LP0ml0g0rEwg",
    "outputId": "311fd76b-e04b-4436-e6b3-4ec3f2b9ad90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_model.load_state_dict(torch.load('/content/drive/MyDrive/multi_model_SA/TFN_MAE-1.4939_50epochs.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_C-rC-Itq9G"
   },
   "outputs": [],
   "source": [
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(final_model.parameters())[2:],lr=5e-4,weight_decay=0.01)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eAovmM4tcIp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PHS0TU9AtcIp",
    "outputId": "41f391ce-082f-42ae-eca4-56653dfaaec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH :  1\n",
      "-------------Training----------------\n",
      "Epoch [1/100],MAE:0.7785 ,r:0.7558\n",
      "--------------Validation----------\n",
      "Epoch [1/100],mae: 1.0989,r:0.5594\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  2\n",
      "-------------Training----------------\n",
      "Epoch [2/100],MAE:0.7123 ,r:0.7945\n",
      "--------------Validation----------\n",
      "Epoch [2/100],mae: 1.0746,r:0.5737\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  3\n",
      "-------------Training----------------\n",
      "Epoch [3/100],MAE:0.7055 ,r:0.7964\n",
      "--------------Validation----------\n",
      "Epoch [3/100],mae: 1.0462,r:0.5673\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  4\n",
      "-------------Training----------------\n",
      "Epoch [4/100],MAE:0.6878 ,r:0.8073\n",
      "--------------Validation----------\n",
      "Epoch [4/100],mae: 1.0908,r:0.5517\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  5\n",
      "-------------Training----------------\n",
      "Epoch [5/100],MAE:0.6801 ,r:0.8073\n",
      "--------------Validation----------\n",
      "Epoch [5/100],mae: 1.0517,r:0.5739\n",
      "\n",
      " \n",
      "\n",
      "EPOCH :  6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-87a1f2c26ca6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(\"EPOCH : \",epoch+1)\n",
    "    # Training\n",
    "    total_train_loss=0.0\n",
    "    num_sequences=0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    final_model.train()  # Set the model to training mode\n",
    "    for batch in traindata:\n",
    "        # targets=targets.unsqueeze(1).repeat(1, 50, 1)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        labels=batch[-1]\n",
    "        outputs = final_model(batch[:-1])\n",
    "        loss = Loss(outputs, batch[-1])\n",
    "\n",
    "        total_train_loss+=loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        all_predictions.extend(outputs)\n",
    "        all_targets.extend(labels)\n",
    "        num_sequences+=1\n",
    "\n",
    "    all_predictions = np.array([tensor.detach().numpy() for tensor in all_predictions])\n",
    "    all_targets = np.array([tensor.detach().numpy() for tensor in all_targets])\n",
    "\n",
    "    average_train_loss = total_train_loss / num_sequences\n",
    "    train_mae= mean_absolute_error(all_targets, all_predictions)\n",
    "    r = scipy.stats.pearsonr(all_targets.ravel(), all_predictions.ravel())\n",
    "    print(\"-------------Training----------------\")\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}],MAE:{train_mae:.4f} ,r:{r[0]:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    final_model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    num_sequences=0\n",
    "    val_all_predictions = []\n",
    "    val_all_targets = []\n",
    "    with torch.no_grad():\n",
    "        best_val_loss = np.inf\n",
    "        patience=3\n",
    "        current_patience = patience\n",
    "\n",
    "        for batch in validdata:\n",
    "            val_targets=batch[-1]\n",
    "            val_outputs = final_model(batch[:-1])\n",
    "            val_loss = Loss(val_outputs, val_targets)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # val_targets_np = val_targets.numpy()\n",
    "            # val_outputs_np = val_outputs.numpy()\n",
    "\n",
    "\n",
    "            val_all_predictions.extend(val_outputs)\n",
    "            val_all_targets.extend(val_targets)\n",
    "            num_sequences+=1\n",
    "\n",
    "    average_val_loss = total_val_loss / num_sequences\n",
    "    val_all_predictions=np.array(val_all_predictions)\n",
    "    val_all_targets=np.array(val_all_targets)\n",
    "    mae = mean_absolute_error(val_all_targets, val_all_predictions)\n",
    "    val_r = scipy.stats.pearsonr(val_all_targets.ravel(), val_all_predictions.ravel())\n",
    "\n",
    "    # print(val_all_predictions.shape)\n",
    "    # print(val_all_targets.shape)\n",
    "\n",
    "    # print(val_all_predictions)\n",
    "    # print(val_all_targets)\n",
    "\n",
    "    # r=np.corrcoef(val_all_targets, val_all_predictions)[0,1]\n",
    "\n",
    "    print(\"--------------Validation----------\")\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}],mae: {mae:.4f},r:{val_r[0]:.4f}')\n",
    "\n",
    "    # if average_val_loss < best_val_loss:\n",
    "    #     best_val_loss=average_val_loss\n",
    "    #     current_patience=patience\n",
    "    # else:\n",
    "    #     current_patience-=1\n",
    "    #     if current_patience==0:\n",
    "    #       print(\"Model performance degarding , Early stopping!!\")\n",
    "    #       break\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eK1pV2IPtcIp",
    "outputId": "b78145e8-4846-4c03-8899-b3da2fae803d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 1.2056 , r:0.4589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "final_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in testdata:\n",
    "        test_targets=batch[-1]\n",
    "        test_outputs = final_model(batch[:-1])\n",
    "\n",
    "        all_predictions.extend(test_outputs)\n",
    "        all_targets.extend(test_targets)\n",
    "\n",
    "    all_predictions=np.array(all_predictions)\n",
    "    all_targets=np.array(all_targets)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    r =  scipy.stats.pearsonr(all_targets.ravel(), all_predictions.ravel())\n",
    "\n",
    "    print(f'Test MAE: {mae:.4f} , r:{r[0]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9fPZKKktgNG"
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt2MvMvXth_j"
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sxfFGetBw4M"
   },
   "source": [
    "### **keeping everything according to paper:**\n",
    "\n",
    "1) lr=0.0005 , weight_decay=0.01 , 100 epochs(50/100) - mae = 1.28 on val and 1.49 on test\n",
    "\n",
    "### **not keeping everything same as paper:**\n",
    "\n",
    "added dropout to text subnetwork along with others\n",
    "\n",
    "1) lr=0.0005 , weight_decay=0.01 , 100 epochs(40/100) - mae = 1.33 on val and 1.54 on test\n",
    "\n",
    "added dropout to text subnetwork and removed from sentiment\n",
    "\n",
    "2) lr=0.0005 , weight_decay=0.01 , 100 epochs(40/100) - mae = 1.34 on val and 1.55 on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2WUXu0OYyqJ"
   },
   "source": [
    "# Saving the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8Rlev7zY02x"
   },
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), '/content/drive/MyDrive/multi_model_SA/TFN_MAE-1.2386_R-0.0171.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-1AKsBPh_nM"
   },
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), '/content/drive/MyDrive/multi_model_SA/TFN_MAE-1.4939_50epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5wYH_ThV2nt"
   },
   "source": [
    "------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
